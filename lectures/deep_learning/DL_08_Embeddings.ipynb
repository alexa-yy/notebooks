{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/DL.png\" width=110 align=\"left\" style=\"margin-right: 10px\">\n",
    "\n",
    "# Introduction to Deep Learning\n",
    "\n",
    "## 08. Embeddings\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Word](https://keras.io/layers/embeddings/) [Embeddings](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "\n",
    "<div style=\"display: inline-block;\">\n",
    "<img src=\"https://adriancolyer.files.wordpress.com/2016/04/word2vec-king-queen-vectors.png?w=566&zoom=2\" width=400 align='left'>\n",
    "<img src=\"https://adriancolyer.files.wordpress.com/2016/04/word2vec-king-queen-composition.png?w=566&zoom=2\" width=400 align='left'>\n",
    "</div>\n",
    "\n",
    "<div style='align: clear'/>\n",
    "<br>Images from <a href=\"https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/\">the morning paper</a>\n",
    "\n",
    "> _Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with a much lower dimension._  \n",
    "_Methods to generate this mapping include neural networks, dimensionality reduction on the word co-occurrence matrix, probabilistic models, explainable knowledge base method, and explicit representation in terms of the context in which words appear._ - [source](https://en.wikipedia.org/wiki/Word_embedding)\n",
    "\n",
    "The intuition to the model is that words with similar contexts have similar meaning.\n",
    "\n",
    "#### Training\n",
    "\n",
    "<div style=\"display: inline-block;\">\n",
    "<img src=\"http://mccormickml.com/assets/word2vec/training_data.png\" width=300 align='left'>\n",
    "<img src=\"http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png\" width=300 align='left'>\n",
    "<img src=\"http://mccormickml.com/assets/word2vec/word2vec_weight_matrix_lookup_table.png\" width=300 align='left'>\n",
    "</div>\n",
    "\n",
    "<div style='align: clear'/>\n",
    "<br>Images from <a href=\"http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\">Word2Vec Tutorial - The Skip-Gram Model.</a>, by <a href=\"http://mccormickml.com/\">Chris McCormick</a>\n",
    "\n",
    "There are two approach to learn word-embeddings: \n",
    "- the continous bag-of-words (CBOW): the model predicts the selected word from the context words in the surrounding window (word order invariant)\n",
    "- the skip-gram architecture:  the model predicts the context words from the selected word (context words are weighted by their distance to the selected word)\n",
    "\n",
    "\n",
    "#### Further reading:\n",
    "\n",
    "- https://www.tensorflow.org/tutorials/representation/word2vec#vector-representations-of-words\n",
    "- https://www.quora.com/How-does-word2vec-work-Can-someone-walk-through-a-specific-example\n",
    "- http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "- https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa\n",
    "- https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526\n",
    "- https://hackernoon.com/word-embeddings-in-nlp-and-its-applications-fab15eaf7430\n",
    "- https://blog.cambridgespark.com/tutorial-build-your-own-embedding-and-use-it-in-a-neural-network-e9cde4a81296\n",
    "- https://skymind.ai/wiki/word2vec\n",
    "- https://github.com/anvaka/word2vec-graph\n",
    "- https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "- https://heartbeat.fritz.ai/using-a-keras-embedding-layer-to-handle-text-data-2c88dc019600\n",
    "- [Google Word2Vec](https://code.google.com/archive/p/word2vec/)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### In Practice\n",
    "\n",
    "#### Learning simple word-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['Well done!',\n",
    "        'Good work',\n",
    "        'Great effort',\n",
    "        'nice work',\n",
    "        'Excellent!',\n",
    "        'Weak',\n",
    "        'Poor effort!',\n",
    "        'not good',\n",
    "        'poor work',\n",
    "        'Could have done better.']\n",
    "\n",
    "labels = np.array([1, 1, 1, 1, 1,\n",
    "                   0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode the documents\n",
    "vocab_size = 50\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 8, input_length=max_length),\n",
    "    Flatten(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "score = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('loss: {}, accuracy: {}'.format(*score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Classify the 20newsgroups dataset while building an embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Further tutorials:\n",
    "- https://www.pyimagesearch.com/2018/09/10/keras-tutorial-how-to-get-started-with-keras-deep-learning-and-python/\n",
    "- https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/\n",
    "- https://www.datacamp.com/community/tutorials/deep-learning-python\n",
    "- https://elitedatascience.com/keras-tutorial-deep-learning-in-python\n",
    "- https://www.guru99.com/keras-tutorial.html\n",
    "- https://github.com/adventuresinML/adventures-in-ml-code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
