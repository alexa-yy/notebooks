{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/DL.png\" width=110 align=\"left\" style=\"margin-right: 10px\">\n",
    "\n",
    "# Introduction to Deep Learning\n",
    "\n",
    "## 09. Reinforcement Learning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GYM random Mountain Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0:\n",
      "action: 1\n",
      "observation: [-0.44385193 -0.00059676]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 1:\n",
      "action: 1\n",
      "observation: [-0.4450411  -0.00118917]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 2:\n",
      "action: 2\n",
      "observation: [-0.44581402 -0.00077292]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 3:\n",
      "action: 1\n",
      "observation: [-0.44716504 -0.00135102]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 4:\n",
      "action: 2\n",
      "observation: [-0.4480843  -0.00091926]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 5:\n",
      "action: 0\n",
      "observation: [-0.45056509 -0.00248079]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 6:\n",
      "action: 1\n",
      "observation: [-0.45358926 -0.00302417]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 7:\n",
      "action: 2\n",
      "observation: [-0.45613465 -0.00254539]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 8:\n",
      "action: 1\n",
      "observation: [-0.45918257 -0.00304792]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 9:\n",
      "action: 1\n",
      "observation: [-0.46271062 -0.00352804]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 10:\n",
      "action: 1\n",
      "observation: [-0.46669279 -0.00398217]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 11:\n",
      "action: 2\n",
      "observation: [-0.47009968 -0.00340689]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 12:\n",
      "action: 1\n",
      "observation: [-0.4739061  -0.00380642]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 13:\n",
      "action: 0\n",
      "observation: [-0.47908383 -0.00517773]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 14:\n",
      "action: 2\n",
      "observation: [-0.48359444 -0.0045106 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 15:\n",
      "action: 2\n",
      "observation: [-0.48740435 -0.00380992]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 16:\n",
      "action: 0\n",
      "observation: [-0.4924852  -0.00508084]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 17:\n",
      "action: 2\n",
      "observation: [-0.49679905 -0.00431386]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 18:\n",
      "action: 2\n",
      "observation: [-0.50031369 -0.00351464]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 19:\n",
      "action: 2\n",
      "observation: [-0.50300282 -0.00268913]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 20:\n",
      "action: 1\n",
      "observation: [-0.50584632 -0.0028435 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 21:\n",
      "action: 0\n",
      "observation: [-0.50982291 -0.00397658]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 22:\n",
      "action: 2\n",
      "observation: [-0.51290278 -0.00307987]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 23:\n",
      "action: 0\n",
      "observation: [-0.51706286 -0.00416008]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 24:\n",
      "action: 2\n",
      "observation: [-0.52027196 -0.0032091 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 25:\n",
      "action: 1\n",
      "observation: [-0.52350601 -0.00323405]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 26:\n",
      "action: 2\n",
      "observation: [-0.52574075 -0.00223474]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 27:\n",
      "action: 0\n",
      "observation: [-0.52895943 -0.00321868]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 28:\n",
      "action: 0\n",
      "observation: [-0.5331379  -0.00417848]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 29:\n",
      "action: 2\n",
      "observation: [-0.53624485 -0.00310694]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 30:\n",
      "action: 1\n",
      "observation: [-0.53925696 -0.00301212]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 31:\n",
      "action: 1\n",
      "observation: [-0.54215169 -0.00289473]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 32:\n",
      "action: 2\n",
      "observation: [-0.54390734 -0.00175565]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 33:\n",
      "action: 1\n",
      "observation: [-0.54551077 -0.00160343]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 34:\n",
      "action: 2\n",
      "observation: [-5.45949979e-01 -4.39208749e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 35:\n",
      "action: 1\n",
      "observation: [-5.46221679e-01 -2.71700317e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 36:\n",
      "action: 1\n",
      "observation: [-5.46323838e-01 -1.02158767e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 37:\n",
      "action: 2\n",
      "observation: [-0.54525569  0.00106815]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 38:\n",
      "action: 2\n",
      "observation: [-0.54302523  0.00223046]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 39:\n",
      "action: 0\n",
      "observation: [-0.54164916  0.00137608]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 40:\n",
      "action: 0\n",
      "observation: [-5.41137768e-01  5.11387460e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 41:\n",
      "action: 2\n",
      "observation: [-0.5394949   0.00164287]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 42:\n",
      "action: 0\n",
      "observation: [-0.53873285  0.00076204]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 43:\n",
      "action: 2\n",
      "observation: [-0.53685734  0.00187551]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 44:\n",
      "action: 0\n",
      "observation: [-0.53588242  0.00097492]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 45:\n",
      "action: 2\n",
      "observation: [-0.53381539  0.00206703]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 46:\n",
      "action: 1\n",
      "observation: [-0.53167174  0.00214364]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 47:\n",
      "action: 1\n",
      "observation: [-0.52946756  0.00220418]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 48:\n",
      "action: 2\n",
      "observation: [-0.52621936  0.0032482 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 49:\n",
      "action: 0\n",
      "observation: [-0.52395151  0.00226785]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 50:\n",
      "action: 1\n",
      "observation: [-0.52168101  0.0022705 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 51:\n",
      "action: 2\n",
      "observation: [-0.51842489  0.00325612]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 52:\n",
      "action: 2\n",
      "observation: [-0.51420758  0.00421731]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 53:\n",
      "action: 0\n",
      "observation: [-0.51106069  0.00314689]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 54:\n",
      "action: 2\n",
      "observation: [-0.50700782  0.00405287]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 55:\n",
      "action: 1\n",
      "observation: [-0.50307933  0.00392849]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 56:\n",
      "action: 1\n",
      "observation: [-0.49930463  0.00377469]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 57:\n",
      "action: 1\n",
      "observation: [-0.49571198  0.00359265]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 58:\n",
      "action: 2\n",
      "observation: [-0.49132824  0.00438374]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 59:\n",
      "action: 0\n",
      "observation: [-0.48818615  0.00314209]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 60:\n",
      "action: 0\n",
      "observation: [-0.48630915  0.001877  ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 61:\n",
      "action: 0\n",
      "observation: [-0.48571124  0.00059791]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 62:\n",
      "action: 2\n",
      "observation: [-0.48439688  0.00131436]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 63:\n",
      "action: 2\n",
      "observation: [-0.48237585  0.00202103]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 64:\n",
      "action: 1\n",
      "observation: [-0.48066321  0.00171264]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 65:\n",
      "action: 1\n",
      "observation: [-0.4792717   0.00139151]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 66:\n",
      "action: 2\n",
      "observation: [-0.47721166  0.00206004]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 67:\n",
      "action: 0\n",
      "observation: [-0.4764984   0.00071326]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 68:\n",
      "action: 0\n",
      "observation: [-0.47713722 -0.00063882]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 69:\n",
      "action: 0\n",
      "observation: [-0.47912338 -0.00198615]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 70:\n",
      "action: 1\n",
      "observation: [-0.48144211 -0.00231873]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 71:\n",
      "action: 0\n",
      "observation: [-0.48507617 -0.00363406]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 72:\n",
      "action: 0\n",
      "observation: [-0.48999851 -0.00492234]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 73:\n",
      "action: 2\n",
      "observation: [-0.49417243 -0.00417392]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 74:\n",
      "action: 2\n",
      "observation: [-0.49756676 -0.00339433]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 75:\n",
      "action: 2\n",
      "observation: [-0.50015613 -0.00258937]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 76:\n",
      "action: 2\n",
      "observation: [-0.50192117 -0.00176504]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 77:\n",
      "action: 0\n",
      "observation: [-0.50484868 -0.00292751]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 78:\n",
      "action: 0\n",
      "observation: [-0.50891675 -0.00406806]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 79:\n",
      "action: 2\n",
      "observation: [-0.51209489 -0.00317814]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 80:\n",
      "action: 0\n",
      "observation: [-0.51635929 -0.0042644 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 81:\n",
      "action: 1\n",
      "observation: [-0.52067799 -0.0043187 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 82:\n",
      "action: 0\n",
      "observation: [-0.52601859 -0.0053406 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 83:\n",
      "action: 0\n",
      "observation: [-0.53234105 -0.00632245]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 84:\n",
      "action: 1\n",
      "observation: [-0.53859794 -0.00625689]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 85:\n",
      "action: 0\n",
      "observation: [-0.54574238 -0.00714444]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 86:\n",
      "action: 0\n",
      "observation: [-0.55372086 -0.00797848]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 87:\n",
      "action: 1\n",
      "observation: [-0.56147374 -0.00775288]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 88:\n",
      "action: 0\n",
      "observation: [-0.56994316 -0.00846942]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 89:\n",
      "action: 1\n",
      "observation: [-0.57806612 -0.00812296]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 90:\n",
      "action: 1\n",
      "observation: [-0.58578239 -0.00771627]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 91:\n",
      "action: 1\n",
      "observation: [-0.59303499 -0.00725259]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 92:\n",
      "action: 1\n",
      "observation: [-0.59977057 -0.00673558]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 93:\n",
      "action: 1\n",
      "observation: [-0.60593982 -0.00616925]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 94:\n",
      "action: 0\n",
      "observation: [-0.61249777 -0.00655795]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 95:\n",
      "action: 0\n",
      "observation: [-0.61939686 -0.00689909]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 96:\n",
      "action: 0\n",
      "observation: [-0.62658731 -0.00719045]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 97:\n",
      "action: 2\n",
      "observation: [-0.63201758 -0.00543027]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 98:\n",
      "action: 2\n",
      "observation: [-0.63564897 -0.00363139]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 99:\n",
      "action: 0\n",
      "observation: [-0.63945572 -0.00380675]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 100:\n",
      "action: 1\n",
      "observation: [-0.64241093 -0.00295521]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 101:\n",
      "action: 1\n",
      "observation: [-0.6444938  -0.00208287]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 102:\n",
      "action: 0\n",
      "observation: [-0.64668971 -0.00219591]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 103:\n",
      "action: 1\n",
      "observation: [-0.64798327 -0.00129356]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 104:\n",
      "action: 1\n",
      "observation: [-6.48365447e-01 -3.82178159e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 105:\n",
      "action: 0\n",
      "observation: [-6.48833572e-01 -4.68125416e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 106:\n",
      "action: 2\n",
      "observation: [-0.64738438  0.00144919]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 107:\n",
      "action: 1\n",
      "observation: [-0.64502798  0.00235639]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 108:\n",
      "action: 1\n",
      "observation: [-0.64178088  0.0032471 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 109:\n",
      "action: 1\n",
      "observation: [-0.63766586  0.00411502]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 110:\n",
      "action: 1\n",
      "observation: [-0.63271194  0.00495392]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 111:\n",
      "action: 1\n",
      "observation: [-0.62695421  0.00575773]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 112:\n",
      "action: 1\n",
      "observation: [-0.62043367  0.00652054]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 113:\n",
      "action: 1\n",
      "observation: [-0.61319705  0.00723663]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 114:\n",
      "action: 0\n",
      "observation: [-0.6062965   0.00690055]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 115:\n",
      "action: 0\n",
      "observation: [-0.59978206  0.00651444]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 116:\n",
      "action: 0\n",
      "observation: [-0.5937012   0.00608085]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 117:\n",
      "action: 2\n",
      "observation: [-0.58609845  0.00760275]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 118:\n",
      "action: 2\n",
      "observation: [-0.57702969  0.00906876]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 119:\n",
      "action: 2\n",
      "observation: [-0.56656191  0.01046778]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 120:\n",
      "action: 1\n",
      "observation: [-0.5557728   0.01078911]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 121:\n",
      "action: 1\n",
      "observation: [-0.54474276  0.01103004]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 122:\n",
      "action: 2\n",
      "observation: [-0.53255424  0.01218851]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 123:\n",
      "action: 1\n",
      "observation: [-0.52029857  0.01225567]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 124:\n",
      "action: 2\n",
      "observation: [-0.50706765  0.01323092]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 125:\n",
      "action: 2\n",
      "observation: [-0.49296066  0.01410699]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 126:\n",
      "action: 2\n",
      "observation: [-0.47808313  0.01487753]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 127:\n",
      "action: 0\n",
      "observation: [-0.46454591  0.01353722]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 128:\n",
      "action: 1\n",
      "observation: [-0.45144928  0.01309664]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 129:\n",
      "action: 0\n",
      "observation: [-0.43988955  0.01155973]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 130:\n",
      "action: 0\n",
      "observation: [-0.42995106  0.00993849]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 131:\n",
      "action: 2\n",
      "observation: [-0.41970572  0.01024533]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 132:\n",
      "action: 1\n",
      "observation: [-0.41022703  0.00947869]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 133:\n",
      "action: 0\n",
      "observation: [-0.40258233  0.0076447 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 134:\n",
      "action: 1\n",
      "observation: [-0.39582545  0.00675689]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 135:\n",
      "action: 2\n",
      "observation: [-0.38900357  0.00682188]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 136:\n",
      "action: 1\n",
      "observation: [-0.38316394  0.00583963]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 137:\n",
      "action: 1\n",
      "observation: [-0.37834669  0.00481725]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 138:\n",
      "action: 0\n",
      "observation: [-0.37558469  0.00276201]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 139:\n",
      "action: 2\n",
      "observation: [-0.37289666  0.00268802]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 140:\n",
      "action: 2\n",
      "observation: [-0.37030079  0.00259587]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 141:\n",
      "action: 2\n",
      "observation: [-0.36781455  0.00248624]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 142:\n",
      "action: 2\n",
      "observation: [-0.36545463  0.00235993]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 143:\n",
      "action: 2\n",
      "observation: [-0.36323678  0.00221784]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 144:\n",
      "action: 2\n",
      "observation: [-0.36117579  0.00206099]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 145:\n",
      "action: 1\n",
      "observation: [-0.36028534  0.00089045]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 146:\n",
      "action: 0\n",
      "observation: [-0.36157132 -0.00128598]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 147:\n",
      "action: 0\n",
      "observation: [-0.36502522 -0.0034539 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 148:\n",
      "action: 2\n",
      "observation: [-0.36862406 -0.00359884]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 149:\n",
      "action: 1\n",
      "observation: [-0.3733438  -0.00471973]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 150:\n",
      "action: 1\n",
      "observation: [-0.37915266 -0.00580887]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 151:\n",
      "action: 1\n",
      "observation: [-0.38601129 -0.00685862]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 152:\n",
      "action: 2\n",
      "observation: [-0.39287276 -0.00686148]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 153:\n",
      "action: 0\n",
      "observation: [-0.40168974 -0.00881698]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 154:\n",
      "action: 2\n",
      "observation: [-0.4104008  -0.00871105]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 155:\n",
      "action: 2\n",
      "observation: [-0.41894461 -0.00854381]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 156:\n",
      "action: 0\n",
      "observation: [-0.4292605  -0.01031589]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 157:\n",
      "action: 0\n",
      "observation: [-0.44127452 -0.01201402]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 158:\n",
      "action: 0\n",
      "observation: [-0.45489971 -0.01362519]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 159:\n",
      "action: 1\n",
      "observation: [-0.46903651 -0.0141368 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 160:\n",
      "action: 2\n",
      "observation: [-0.48258069 -0.01354419]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 161:\n",
      "action: 2\n",
      "observation: [-0.49543174 -0.01285105]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 162:\n",
      "action: 2\n",
      "observation: [-0.50749379 -0.01206205]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 163:\n",
      "action: 0\n",
      "observation: [-0.52067658 -0.01318279]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 164:\n",
      "action: 2\n",
      "observation: [-0.53288128 -0.01220471]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 165:\n",
      "action: 2\n",
      "observation: [-0.54401638 -0.0111351 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 166:\n",
      "action: 0\n",
      "observation: [-0.55599844 -0.01198206]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 167:\n",
      "action: 2\n",
      "observation: [-0.56673788 -0.01073944]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 168:\n",
      "action: 1\n",
      "observation: [-0.57715468 -0.0104168 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 169:\n",
      "action: 0\n",
      "observation: [-0.58817154 -0.01101686]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 170:\n",
      "action: 0\n",
      "observation: [-0.59970713 -0.01153559]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 171:\n",
      "action: 2\n",
      "observation: [-0.60967685 -0.00996972]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 172:\n",
      "action: 0\n",
      "observation: [-0.62000814 -0.01033129]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 173:\n",
      "action: 0\n",
      "observation: [-0.6306264  -0.01061826]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 174:\n",
      "action: 1\n",
      "observation: [-0.64045567 -0.00982927]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 175:\n",
      "action: 0\n",
      "observation: [-0.65042635 -0.00997069]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 176:\n",
      "action: 0\n",
      "observation: [-0.66046862 -0.01004226]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 177:\n",
      "action: 0\n",
      "observation: [-0.67051296 -0.01004434]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 178:\n",
      "action: 0\n",
      "observation: [-0.68049078 -0.00997782]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 179:\n",
      "action: 2\n",
      "observation: [-0.68833487 -0.00784409]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 180:\n",
      "action: 0\n",
      "observation: [-0.69599312 -0.00765826]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 181:\n",
      "action: 2\n",
      "observation: [-0.7014153  -0.00542217]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 182:\n",
      "action: 2\n",
      "observation: [-0.7045662  -0.00315091]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 183:\n",
      "action: 0\n",
      "observation: [-0.70742555 -0.00285935]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 184:\n",
      "action: 0\n",
      "observation: [-0.70997503 -0.00254948]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 185:\n",
      "action: 0\n",
      "observation: [-0.71219838 -0.00222336]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 186:\n",
      "action: 0\n",
      "observation: [-0.71408151 -0.00188313]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 187:\n",
      "action: 0\n",
      "observation: [-0.71561252 -0.001531  ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 188:\n",
      "action: 2\n",
      "observation: [-0.71478175  0.00083077]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 189:\n",
      "action: 0\n",
      "observation: [-0.71359444  0.00118731]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 190:\n",
      "action: 1\n",
      "observation: [-0.71105808  0.00253636]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 191:\n",
      "action: 2\n",
      "observation: [-0.70618872  0.00486936]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 192:\n",
      "action: 1\n",
      "observation: [-0.7000174   0.00617132]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 193:\n",
      "action: 2\n",
      "observation: [-0.69158385  0.00843355]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 194:\n",
      "action: 1\n",
      "observation: [-0.68194307  0.00964078]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 195:\n",
      "action: 2\n",
      "observation: [-0.67015887  0.0117842 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 196:\n",
      "action: 0\n",
      "observation: [-0.65831055  0.01184833]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 197:\n",
      "action: 1\n",
      "observation: [-0.64547916  0.01283139]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 198:\n",
      "action: 1\n",
      "observation: [-0.6317539   0.01372526]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 199:\n",
      "action: 0\n",
      "observation: [-0.61823164  0.01352226]\n",
      "reward: -1.0\n",
      "done: True\n",
      "info: {'TimeLimit.truncated': True}\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "for step_index in range(1000):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    print(\"Step {}:\".format(step_index))\n",
    "    print(\"action: {}\".format(action))\n",
    "    print(\"observation: {}\".format(observation))\n",
    "    print(\"reward: {}\".format(reward))\n",
    "    print(\"done: {}\".format(done))\n",
    "    print(\"info: {}\".format(info))\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.tanka.la/2018/10/19/solving-curious-case-of-mountaincar-reward-problem-using-openai-gym-keras-tensorflow-in-python/\n",
    "https://towardsdatascience.com/reinforcement-learning-w-keras-openai-dqns-1eed3a5338c\n",
    "https://github.com/pylSER/Deep-Reinforcement-learning-Mountain-Car\n",
    "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "goal_steps = 200\n",
    "score_requirement = -198\n",
    "intial_games = 10000\n",
    "\n",
    "def model_data_preparation():\n",
    "    training_data = []\n",
    "    accepted_scores = []\n",
    "    for game_index in range(intial_games):\n",
    "        score = 0\n",
    "        game_memory = []\n",
    "        previous_observation = []\n",
    "        for step_index in range(goal_steps):\n",
    "            action = random.randrange(0, 3)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            \n",
    "            if len(previous_observation) > 0:\n",
    "                game_memory.append([previous_observation, action])\n",
    "                \n",
    "            previous_observation = observation\n",
    "            if observation[0] > -0.2:\n",
    "                reward = 1\n",
    "            \n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "        if score >= score_requirement:\n",
    "            accepted_scores.append(score)\n",
    "            for data in game_memory:\n",
    "                if data[1] == 1:\n",
    "                    output = [0, 1, 0]\n",
    "                elif data[1] == 0:\n",
    "                    output = [1, 0, 0]\n",
    "                elif data[1] == 2:\n",
    "                    output = [0, 0, 1]\n",
    "                training_data.append([data[0], output])\n",
    "        \n",
    "        env.reset()\n",
    "    \n",
    "    print(accepted_scores)\n",
    "    \n",
    "    return training_data\n",
    "\n",
    "\n",
    "def build_model(input_size, output_size):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=input_size, activation='relu'))\n",
    "    model.add(Dense(52, activation='relu'))\n",
    "    model.add(Dense(output_size, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=Adam())\n",
    "    return model\n",
    "\n",
    "def train_model(training_data):\n",
    "    X = np.array([i[0] for i in training_data]).reshape(-1, len(training_data[0][0]))\n",
    "    y = np.array([i[1] for i in training_data]).reshape(-1, len(training_data[0][1]))\n",
    "    model = build_model(input_size=len(X[0]), output_size=len(y[0]))\n",
    "    \n",
    "    model.fit(X, y, epochs=10)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-188.0, -168.0, -172.0, -198.0, -196.0, -176.0, -194.0, -178.0, -192.0, -176.0, -176.0, -176.0, -186.0, -182.0, -172.0, -178.0, -184.0, -198.0, -172.0, -172.0, -180.0, -198.0, -174.0, -166.0, -176.0, -174.0, -192.0, -178.0, -174.0, -192.0, -172.0, -182.0, -178.0, -176.0, -172.0, -192.0, -170.0, -194.0, -180.0, -188.0, -190.0, -192.0, -174.0, -188.0, -184.0, -182.0, -168.0]\n",
      "Epoch 1/10\n",
      "9353/9353 [==============================] - 0s 38us/step - loss: 0.2283\n",
      "Epoch 2/10\n",
      "9353/9353 [==============================] - 0s 28us/step - loss: 0.2221\n",
      "Epoch 3/10\n",
      "9353/9353 [==============================] - 0s 27us/step - loss: 0.2217\n",
      "Epoch 4/10\n",
      "9353/9353 [==============================] - 0s 26us/step - loss: 0.2209\n",
      "Epoch 5/10\n",
      "9353/9353 [==============================] - 0s 27us/step - loss: 0.2205\n",
      "Epoch 6/10\n",
      "9353/9353 [==============================] - 0s 28us/step - loss: 0.2204\n",
      "Epoch 7/10\n",
      "9353/9353 [==============================] - 0s 26us/step - loss: 0.2201\n",
      "Epoch 8/10\n",
      "9353/9353 [==============================] - 0s 26us/step - loss: 0.2202\n",
      "Epoch 9/10\n",
      "9353/9353 [==============================] - 0s 27us/step - loss: 0.2200\n",
      "Epoch 10/10\n",
      "9353/9353 [==============================] - 0s 29us/step - loss: 0.2198\n"
     ]
    }
   ],
   "source": [
    "training_data = model_data_preparation()\n",
    "trained_model = train_model(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-131.0, -121.0, -87.0, -121.0, -129.0, -116.0, -87.0, -131.0, -127.0, -85.0]\n",
      "Average Score: -113.5\n",
      "choice 1:0.0026431718061674008  choice 0:0.3497797356828194 choice 2:0.6475770925110133\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "choices = []\n",
    "for each_game in range(10):\n",
    "    score = 0\n",
    "    game_memory = []\n",
    "    prev_obs = []\n",
    "    for step_index in range(goal_steps):\n",
    "        env.render()\n",
    "        if len(prev_obs)==0:\n",
    "            action = random.randrange(0,2)\n",
    "        else:\n",
    "            action = np.argmax(trained_model.predict(prev_obs.reshape(-1, len(prev_obs)))[0])\n",
    "        \n",
    "        choices.append(action)\n",
    "        new_observation, reward, done, info = env.step(action)\n",
    "        prev_obs = new_observation\n",
    "        game_memory.append([new_observation, action])\n",
    "        score += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    env.reset()\n",
    "    scores.append(score)\n",
    "\n",
    "env.close()\n",
    "print(scores)\n",
    "print('Average Score:',sum(scores)/len(scores))\n",
    "print('choice 1:{}  choice 0:{} choice 2:{}'.format(choices.count(1)/len(choices),\n",
    "                                                    choices.count(0)/len(choices),\n",
    "                                                    choices.count(2)/len(choices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Success:\n",
    "\n",
    "    def __init__(self, threshold=10):\n",
    "        self.sum = 0\n",
    "        self.last10 = []\n",
    "        self.last10sum = sum(self.last10)\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def __iadd__(self, value):\n",
    "        self.sum += value\n",
    "        self.last10.append(value)\n",
    "        self.last10 = self.last10[-10:]\n",
    "        self.last10sum = sum(self.last10)\n",
    "        return self\n",
    "\n",
    "    def __add__(self, value):\n",
    "        new = Success()\n",
    "        new.sum = self.sum\n",
    "        new.last10 = self.last10\n",
    "        new.last10sum = self.last10sum\n",
    "        new.threshold = self.threshold\n",
    "        return new.__iadd__(value)\n",
    "\n",
    "    def __bool__(self):\n",
    "        return sum(self.last10) >= self.threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.gamma = 0.99\n",
    "\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = 0.95  # 0.05\n",
    "        self.epsilon_min = 0.01\n",
    "\n",
    "        self.learing_rate = 0.001\n",
    "\n",
    "        self.memory = deque(maxlen=20000)\n",
    "        self.num_episodes = 400\n",
    "        self.max_iter = 201  # max is 200\n",
    "        self.batch_size = 32\n",
    "\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "\n",
    "        self.sync_models()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        state_shape = self.env.observation_space.shape\n",
    "        action_shape = self.env.action_space.n\n",
    "\n",
    "        model.add(Dense(24, activation='relu', input_shape=state_shape))\n",
    "        model.add(Dense(48, activation='relu'))\n",
    "        model.add(Dense(action_shape, activation='linear'))\n",
    "        \n",
    "        optimizer = Adam(learning_rate=self.learing_rate)\n",
    "        model.compile(loss='mse', optimizer=optimizer)\n",
    "        return model\n",
    "\n",
    "    def sync_models(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand(1) < self.epsilon:\n",
    "            return np.random.randint(0, 3)\n",
    "        return np.argmax(self.model.predict(state)[0])\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon = max(self.epsilon_min, \n",
    "                               self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.append([state, action, reward, new_state, done])\n",
    "\n",
    "    def generate_batch(self):\n",
    "        samples = np.array(random.sample(self.memory, self.batch_size))\n",
    "        \n",
    "        states, actions, rewards, new_states, dones = np.hsplit(samples, 5)\n",
    "\n",
    "        states = np.concatenate(np.squeeze(states[:]), axis=0) # [batch_size x 2]\n",
    "        new_states = np.concatenate(np.concatenate(new_states)) # [batch_size x 2]\n",
    "        rewards = rewards.reshape(self.batch_size,).astype(float) # [batch_size]\n",
    "        actions = actions.reshape(self.batch_size,).astype(int) # [batch_size]\n",
    "        dones = dones.reshape(self.batch_size,).astype(bool) # [batch_size]\n",
    "        notdones = (~dones).astype(float)\n",
    "    \n",
    "        return states, actions, rewards, new_states, notdones\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, new_states, notdones = self.generate_batch()\n",
    "        targets = self.model.predict(states)\n",
    "        indices = np.arange(self.batch_size)\n",
    "        Q_futures = self.target_model.predict(new_states).max(axis = 1)\n",
    "        targets[(indices, actions)] = rewards + Q_futures * self.gamma * notdones\n",
    "        \n",
    "        self.model.fit(states, targets, epochs=1, verbose=0)\n",
    "\n",
    "    def optimize_model(self, state, eps, render=True):\n",
    "        score = 0\n",
    "        max_position = -99\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            action = self.act(state)\n",
    "\n",
    "            # Show the animation every 50 eps\n",
    "            if render and eps % 50 == 0:\n",
    "                env.render()\n",
    "\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            new_state = new_state.reshape(1, 2)\n",
    "\n",
    "            # Keep track of max position\n",
    "            position = new_state[0][0]\n",
    "            if position > max_position:\n",
    "                max_position = position\n",
    "\n",
    "            # Adjust reward for task completion\n",
    "            if position >= 0.5:\n",
    "                reward += 10\n",
    "\n",
    "            self.remember(state, action, reward, new_state, done)\n",
    "            self.replay()\n",
    "\n",
    "            state = new_state\n",
    "            score += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        self.sync_models()\n",
    "        self.decay_epsilon()\n",
    "        \n",
    "        return i < 199\n",
    "\n",
    "    def fit(self, render=True):\n",
    "        successes = []\n",
    "        success = Success()\n",
    "\n",
    "        episodes = tqdm(range(self.num_episodes))\n",
    "        for eps in episodes:\n",
    "            state = env.reset().reshape(1, 2)\n",
    "            success += self.optimize_model(state, eps, render)\n",
    "\n",
    "            episodes.set_postfix_str(f'overall: {success.sum}, '\n",
    "                                     f'last10: {success.last10sum}')\n",
    "            if success:\n",
    "                print('10 success in a row, stopping early.')\n",
    "                episodes.close()\n",
    "                break\n",
    "        \n",
    "        return self\n",
    "\n",
    "                \n",
    "def play(env, model, n=1):\n",
    "    for _ in range(n):\n",
    "        done = False\n",
    "        state = env.reset().reshape(1, 2)\n",
    "        while not done:\n",
    "            env.render()\n",
    "            action = model.act(state)\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            state = new_state.reshape(1, 2)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e95209a413a4204b06efc33db7be65b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=400.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 success in a row, stopping early.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')            \n",
    "            \n",
    "env.seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "dqn = DQN(env=env).fit(render=False)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(env, dqn, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
