{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Intro to Data Science \n",
    "## Part VII. - Regression and Embedding pipelines\n",
    "\n",
    "### Table of contents\n",
    "\n",
    "- #### Regression\n",
    "    - <a href=\"#What-is-Regression?\">Theory</a>\n",
    "    - <a href=\"#Linear-regression---OLS\">Linear regression</a>\n",
    "    - <a href=\"#Ridge-regression\">Ridge regression</a>\n",
    "    - <a href=\"#LASSO\">LASSO regression</a>\n",
    "    - <a href=\"#Bayesian-Ridge-regression\">Bayesian regression</a>\n",
    "    - <a href=\"#Support-Vector-regression\">Support Vector regression</a>\n",
    "    - <a href=\"#XGBoost\">XGBoost</a>\n",
    "\n",
    "- #### Embedding pipelines\n",
    "    - <a href=\"#Embedding-pipelines\">Reusing trained pipelines</a>\n",
    "        - <a href=\"#Saving-pipelines\">Exporting pipelines</a>\n",
    "        - <a href=\"#Loading-pipelines\">Loading pipelines</a>\n",
    "\n",
    "- #### Kaggle Salary prediction challange - recap\n",
    "\n",
    "---\n",
    "\n",
    "## What is Regression?\n",
    "Regression - just as classification - is a supervised machine learning problem however in case of regression the target variable is continuous. It is also _\"a statistical process for estimating the relationships among variables. It includes many techniques for modeling and analyzing several variables, when the focus is on the relationship between a __dependent variable__ and one or more __independent variable__s (or 'predictors').\"_ from: <a href=\"https://en.wikipedia.org/wiki/Regression_analysis\">Wiki</a>\n",
    "\n",
    "It is important to note that instead of the descriptive nature of statistical regression analysis Data Science focuses on the predictive side of this method.\n",
    "\n",
    "## Why is it important?\n",
    "_\"Regression analysis is widely used for prediction and forecasting, where its use has substantial overlap with the field of machine learning.\"_ from: <a href=\"https://en.wikipedia.org/wiki/Regression_analysis\">Wiki</a>\n",
    "\n",
    "It is used to forecast any continuous variable:\n",
    "- stock market\n",
    "- salary prediction\n",
    "- network traffic\n",
    "- traffic\n",
    "- etc.\n",
    "\n",
    "## Tools\n",
    "- Linear regression\n",
    "- Ridge regression\n",
    "- LASSO\n",
    "- Bayesian regression\n",
    "- Support Vector regression\n",
    "- etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plot_pred(y, predicted):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(y, predicted, edgecolors='k')\n",
    "    ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
    "    ax.set_xlabel('Measured')\n",
    "    ax.set_ylabel('Predicted')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def plot_boston(ax):\n",
    "    ax.scatter(lsop_train, y_train, edgecolors='k', s=10)\n",
    "    ax.set_xlabel(\"% lower status of the population\")\n",
    "    ax.set_ylabel(\"Median value of owner-occupied homes in $1000's\")\n",
    "    ax.set_xlim([-10,50])\n",
    "    ax.set_ylim([0,60])\n",
    "    \n",
    "    \n",
    "def plot_curve(estimator, param, values, ax):   \n",
    "    for color, value in zip(colors, values):\n",
    "        estimator = estimator.set_params(**{param: value}).fit(lsop_train, y_train)\n",
    "        ax.plot(curve_x, estimator.predict(curve_x), '-', c=color, lw=2, label=value)\n",
    "    plot_boston(ax)\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "    \n",
    "def show_score(model, X, y, cv=10, metric=None):\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring=metric)\n",
    "    return \"Accuracy: {:.2f} (+/- {:.2f})\".format(scores.mean(), scores.std() * 2)\n",
    "\n",
    "colors = ['g', 'r', 'y', 'c', 'm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X, y = load_boston(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "lsop = X[:,12][:, np.newaxis]\n",
    "lsop_train = X_train[:,12][:, np.newaxis]\n",
    "lsop_test = X_test[:,12][:, np.newaxis]\n",
    "\n",
    "curve_x = np.linspace(-10,50, num=300)[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "$\\newcommand{\\bs}[1]{\\boldsymbol{#1}}$\n",
    "\n",
    "## Variations on a Theme\n",
    "\n",
    "The traditional linear problem is stated like this:\n",
    "$$ y_i = \\bs{x}_i \\bs{\\beta} $$\n",
    "for every observation $i$, or more compactly\n",
    "$$ \\bs{y} = \\bs{X}\\bs{\\beta} $$\n",
    "where $ \\bs{X} $ is the matrix observed values, $\\bs{y}$ is the vector of observed output variables, and $\\bs{\\beta}$ is the weight vector which we want to find. \n",
    "\n",
    "In OLS, we try to find the $\\bs{\\beta}$ while minimizing a *loss function*, which is simply the sum of squares of the differences between the predicted and observed values (also called sum of squared residuals or SSR), \n",
    "\n",
    "$ \\mathrm{Cost}(\\bs{\\beta}) = \\mathrm{SSR}(\\bs{\\beta}) = \\sum _i (\\hat y_i - y_i)^{2} $.  \n",
    "\n",
    "<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\">Ridge</a>, <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\">LASSO</a> and <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge.html\">Bayesian</a> regressions (and a couple more) are basically simple <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\">linear</a> regressions, but with the loss function being modified.  \n",
    "Ridge regression adds the sum of the squares of the weights with a constant multiplier to the loss, i.e.\n",
    "\n",
    "$ \\mathrm{Cost}(\\bs{\\beta}) = \\sum _i (\\hat y_i - y_i)^{2} + \\alpha \\sum _i \\beta _i^{2}. $\n",
    "\n",
    "LASSO adds the sum of the absolute values of the coefficients, i.e.\n",
    "\n",
    "$ \\mathrm{Cost}(\\bs{\\beta}) = \\sum _i (\\hat y_i - y_i)^{2} + \\alpha \\sum _i \\vert \\beta _i. \\vert $\n",
    "\n",
    "### Ok, but what is the point of this?\n",
    "\n",
    "This technique is called <a href=\"https://en.wikipedia.org/wiki/Regularization_(mathematics)\">**regularization**</a>, and the use of this in our case is to prevent the model from **overfitting** to the data (which is our greatest enemy, right before **the curse of dimensionality**). Basically it prevents the coefficients from growing too large. To illustrate this, we will use the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html\">*boston dataset*</a>. (You should also check out <a href=\"https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/\">this</a> for a more detailed discussion on Ridge and LASSO)\n",
    "\n",
    "---\n",
    "\n",
    "## Linear regression - OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ols = Pipeline([('poly', PolynomialFeatures()), ('ols', LinearRegression())])\n",
    "parameters = {'poly__degree': range(1,16)}\n",
    "ols_grid = GridSearchCV(ols, parameters, n_jobs=2, scoring='neg_mean_squared_error')\n",
    "ols_grid.fit(lsop_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_score(ols_grid.best_estimator_, lsop_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_hat = ols_grid.best_estimator_.predict(lsop_test)\n",
    "plot_pred(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Plot some example curve with different degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_curve(ols, 'poly__degree', [1, 2, 3, 5, 13], ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ridge = Pipeline([('poly', PolynomialFeatures(degree=5)), ('ridge', Ridge())])\n",
    "params = {'ridge__alpha': np.logspace(-15, 13, 29)}\n",
    "ridge_grid = GridSearchCV(ridge, params, n_jobs=2, scoring='neg_mean_squared_error')\n",
    "ridge_grid.fit(lsop_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "show_score(ridge_grid.best_estimator_, lsop_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_hat = ridge_grid.best_estimator_.predict(lsop_test)\n",
    "plot_pred(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Plot some example curves to see how the regularization parameters \"deform\" the 5 degree polynomial we saw in the previous plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_curve(ridge, 'ridge__alpha', [1e-13, 1e-1, 1e1, 1e2, 1e10], ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## LASSO\n",
    "\n",
    "Least absolute shrinkage and selection operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lasso = Pipeline([('poly', PolynomialFeatures(degree=5)), ('lasso', Lasso(max_iter=10000))])\n",
    "params = {'lasso__alpha': np.logspace(-5, 13, 19)}\n",
    "lasso_grid = GridSearchCV(lasso, params, scoring='neg_mean_squared_error')\n",
    "lasso_grid.fit(lsop_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "show_score(lasso_grid.best_estimator_, lsop_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_hat = lasso_grid.best_estimator_.predict(lsop_test)\n",
    "plot_pred(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "LASSO also works as a feature selection tool, we can see that by setting the alpha high enough, it sets some coefficients to zero. Also, we can see that if we go overboard with this, it can lead to **underfitting**, which is also bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "coefs = pd.DataFrame()\n",
    "pipe = Pipeline([('poly', PolynomialFeatures(degree=5)),\n",
    "                 ('lasso', Lasso(max_iter=100000))])\n",
    "for alpha in np.logspace(-5, 13, 19):\n",
    "    pipe = pipe.set_params(lasso__alpha=alpha).fit(lsop_train, y_train)\n",
    "    coefs[alpha] = pipe.named_steps['lasso'].coef_[1:]\n",
    "\n",
    "coefs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_curve(lasso, 'lasso__alpha', [1e-5, 1e-2, 1e-1, 1e1, 1e7], ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import BayesianRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bayes = Pipeline([('poly', PolynomialFeatures(degree=5)), ('bayes', BayesianRidge())])\n",
    "params = {'bayes__alpha_1': np.logspace(-5, 5, 5),\n",
    "          'bayes__alpha_2': np.logspace(-5, 13, 5),\n",
    "          'bayes__lambda_1': np.logspace(-5, 13, 5),\n",
    "          'bayes__lambda_2': np.logspace(-5, 13, 5)}\n",
    "bayes_grid = GridSearchCV(bayes, params, scoring='neg_mean_squared_error')\n",
    "bayes_grid.fit(lsop_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "show_score(bayes_grid.best_estimator_, lsop_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_hat = bayes_grid.best_estimator_.predict(lsop_test)\n",
    "plot_pred(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_curve(bayes, 'bayes__alpha_1', [1e-5, 1e-2, 1e-1, 1e1, 1e2], ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Support Vector regression\n",
    "\n",
    "Support vector machines can be used for regression purposes too. The main idea is to:\n",
    "a) reduce the number of required training points to the support vectors\n",
    "b) fit a linear model\n",
    "c) transform data points into higher dimensions and fit the linear model in that higher space then transform the fitted curve to the original, lower dimension\n",
    "d) instead of actually transforming the data, use kernel functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "svr = SVR(kernel='rbf', C=1e3, gamma=5e-5, degree=2)\n",
    "svr.fit(lsop_train, y_train)\n",
    "show_score(svr, lsop, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_hat = svr.predict(lsop_test)\n",
    "plot_pred(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_curve(svr, 'kernel', ['linear', 'poly', 'rbf'], ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## [XGBoost](https://xgboost.readthedocs.io/en/latest/model.html)\n",
    "\n",
    "XGBoost is short for **Extreme Gradient Boosting** which is a Gradient Boosted Tree method. Boosted tree is an **ensemble method**, basically training multiple trees on the same training set results a more robust solution. It is important that boosted trees incorporates a **regularization term** in its objective function. In this sense, boosted trees are the same as random forests. The difference comes from the training process. \n",
    "\n",
    "XGBoost use additive training: in each step it adds individual trees by selecting the best tree each time. The best tree is the **simplest tree** (tree structure score is minimal) **with the most information gain**.\n",
    "\n",
    "For more detailed explanation please consult with these [slides](http://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf) and this [tutorial](https://xgboost.readthedocs.io/en/latest/model.html) or with this [wiki page](https://en.wikipedia.org/wiki/Gradient_boosting) on gradient boosting. To **install on windows**, please follow the instructions found [here](http://www.picnet.com.au/blogs/guido/post/2016/09/22/xgboost-windows-x64-binaries-for-download/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "xgb = XGBRegressor()\n",
    "xgb.fit(lsop_train, y_train)\n",
    "y_hat = xgb.predict(lsop_test)\n",
    "show_score(xgb, lsop, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plot_pred(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_curve(xgb, 'n_estimators', [1, 5, 10, 25, 100], ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "## Embedding pipelines\n",
    "\n",
    "Trained pipelines can be used outside of the training program as well.\n",
    "\n",
    "### Saving pipelines\n",
    "\n",
    "First, we have to `serialize` the models. This process will save the whole pipeline object into a file. After saving, we can freely move the file and read it in elsewhere.  \n",
    "**Important** to know that the used libraries must be the same versions in the saving and the loading end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('xgboost_model.pickle', 'wb') as picklefile:\n",
    "    pickle.dump(xgb, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Loading pipelines\n",
    "\n",
    "Loading and using the models is pretty easy - as long as we have the same libraries installed (and the same versions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('xgboost_model.pickle', 'rb') as picklefile:\n",
    "    model = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "show_score(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Kaggle challange\n",
    "\n",
    "What to do:\n",
    "\n",
    "- download dataset\n",
    "- determine problem type\n",
    "- examine features (description, insight)\n",
    "- brainstorm possible features\n",
    "- construct pipelines\n",
    "- evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
