{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III. Text mining\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## What is Text Mining?\n",
    "Text mining or text analytics is the process of extracting quantified features from (un)structured (natural language) texts. Processing unstructured data involves using Natural Language Processing (NLP), statistical modeling and machine learning techniques.\n",
    "\n",
    "## Why is it important?\n",
    "80% of the generated data is not available in structured, numerical format (emails, texts, meeting notes, documents, social media feeds). These unstructured data includes images, drawings, videos, voice recordings and unstructured texts. These data can be described with their meta data (length, topic, category, etc.) but transforming them into structured data is important to access the more detailed information stored in such data sources. Voice recordings, videos and drawings can also be transcribed into unstructured texts so they can be processing as textual data. \n",
    "Most common use cases are:\n",
    "\n",
    "- document similarity computation\n",
    "- document deduplication\n",
    "- document clustering\n",
    "- topic extraction\n",
    "- sentiment analysis\n",
    "- automated annotation\n",
    "- text filtering\n",
    "- text classification\n",
    "\n",
    "## Tools\n",
    "- NLP tools\n",
    "    - tokenization\n",
    "    - stemming\n",
    "    - part-of-speech (POS) tagging\n",
    "    - stop word filtering\n",
    "    - bag-of-words representation\n",
    "    - tf-idf transformation\n",
    "- Other tools\n",
    "    - Word2Vec representation\n",
    "    - hashing\n",
    "    - cosine/jaccard/levenshtein/etc similarity computation\n",
    "    - matrix factorization\n",
    "    - etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Mining in practice\n",
    "\n",
    "### 1. Read and examine data\n",
    "\n",
    "Examining unstructured data is the key to proper preprocessing.  \n",
    "The collection of texts is called __`corpus`__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('./data/SMSSpamCollection', encoding='utf-8') as spamfile:\n",
    "    corpus = [line.strip() for line in spamfile]\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for text in corpus[:5]:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the data is in TSV format, read it accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus = pd.read_table('./data/SMSSpamCollection', names=['label', 'message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus['length'] = corpus.message.str.len()\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus.length.plot(bins=20, kind='hist');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus.length.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "910 long sms???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus.loc[corpus.length > 900, 'message'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there a difference between spam and ham messages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus.hist(bins=50, by='label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why not try a simple predictor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "splitted = train_test_split(corpus.length.values[:, np.newaxis], # we need a matrix, not a vector\n",
    "                            corpus.label.values,\n",
    "                            test_size=.25,\n",
    "                            random_state=42)\n",
    "X_train, X_test, y_train, y_test = splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('nb', MultinomialNB())])\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, pipe.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "87% percent is our baseline, let's get into the preprocessing!\n",
    "\n",
    "### 2. Preprocessing\n",
    "#### a) [Bag-of-words representation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)\n",
    "\n",
    "Bag of words representation represent documents as a vector where each different word is represented in a fixed position. The values in the positions are the occurence counts in the given document. For example:\n",
    "The vector features for the documents:\n",
    "```python\n",
    "docs = [\"I like trains.\", \"Trains are like big cars.\", \"I like big cars\"]\n",
    "```\n",
    "will be \n",
    "```python\n",
    "features = {'I': 0, 'like': 1, 'trains': 2, 'are': 3, 'big': 4, 'cars': 5}\n",
    "```\n",
    "and the vectorial representations will be\n",
    "```python\n",
    "vectors = [[1, 1, 1, 0, 0, 0],\n",
    "           [0, 1, 1, 1, 1, 1],\n",
    "           [1, 1, 0, 0, 1, 1]]\n",
    "```\n",
    "\n",
    "Fortunately `scikit-learn` has a built-in solution for this: the [`CountVectorizer`](http://scikit-learn.org/stable/modules/feature_extraction.html#the-bag-of-words-representation).  \n",
    "Let's try out our little example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cntvec = CountVectorizer()\n",
    "docs = [\"I like trains.\",\n",
    "        \"Trains are like big cars.\",\n",
    "        \"I like big cars\"]\n",
    "\n",
    "cntvec.fit_transform(docs).todense(), cntvec.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-grams\n",
    "N-grams are n long word tuples. They are generated by an n long rolling window and they can provide contextual information which sometimes yields better results. An example 2-gram for the `\"I like trains.\"` sentence would be:\n",
    "```python\n",
    "[(\"I\", \"like\"), (\"like\", \"trains\")]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cntvec = CountVectorizer(ngram_range=(2, 2))\n",
    "cntvec.fit_transform(docs).todense(), cntvec.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum and maximum document frequency\n",
    "\n",
    "Minimum and maximum document frequency (`min_df` and `max_df`) are set thresholds to limit the feature numbers. If a _term_ (transformed word) appears less than `min_df` or more than `max_df` times (or percent) then it will be left out from the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cntvec = CountVectorizer(max_df=1)\n",
    "cntvec.fit_transform(docs).todense(), cntvec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cntvec = CountVectorizer(min_df=3)\n",
    "cntvec.fit_transform(docs).todense(), cntvec.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced tokenization\n",
    "\n",
    "In the vocabulary generation process each word are analyzed and transformed in order to reduce vocabulary length. Scikit-learn's default analization function is lowercasing the words and filtering short and stop words but no further transformation is done.\n",
    "\n",
    "NLP has more detailed techniques to better extract the base words. Lemmatization is a powerful tool to reduce a word into it's _root_ form (as it appears in dictionaries): that's how `are` becomes `be` and `trains` becomes `train`, etc.\n",
    "\n",
    "#### Stemming\n",
    "\n",
    "Word stemming means removing affixes from words and return the root word. Stemming do not use contextual information to execute the stripping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmer.stem('trains')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "\n",
    "Word lemmatizing is similar to stemming, but the difference is the result of lemmatizing is a real word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('are', pos='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization has better accuracy but slower than stemming.\n",
    "\n",
    "Using lemmatization we can create custom analyzers to use in CountVectorizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def split_into_lemmas(message):\n",
    "    message = ''.join([char for char in message.lower()\n",
    "                       if char.isalnum() or char.isspace()])\n",
    "    return [lemmatizer.lemmatize(word, pos='v') \n",
    "            for word in message.split()]\n",
    "\n",
    "[split_into_lemmas(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cntvec = CountVectorizer(analyzer=split_into_lemmas)\n",
    "cntvec.fit_transform(docs).todense(), cntvec.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's insert our vectorizer to our pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "splitted = train_test_split(corpus.message,\n",
    "                            corpus.label.values,\n",
    "                            test_size=.25,\n",
    "                            random_state=42)\n",
    "X_train, X_test, y_train, y_test = splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cntvec', CountVectorizer(analyzer=split_into_lemmas, min_df=10, max_df=.5)),\n",
    "                 ('nb', MultinomialNB())])\n",
    "pipe.fit(X_train, y_train)\n",
    "accuracy_score(y_test, pipe.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(pipe.steps[0][-1].vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy, an advanced NLP library\n",
    "\n",
    "There is an advanced library called `spacy` which has more sophisticated tokenization and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "[token.lemma_ for token in nlp(docs[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame([\n",
    "    {'text': token.text, \n",
    "     'lemma': token.lemma_, \n",
    "     'POS': token.pos_, \n",
    "     'tag': token.tag_, \n",
    "     'dep': token.dep_,\n",
    "     'shape': token.shape_,\n",
    "     'is_alpha': token.is_alpha, \n",
    "     'is_stop': token.is_stop}\n",
    "    for token in nlp(docs[0])\n",
    "]).set_index('text').transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cntvec', CountVectorizer(analyzer=lambda x: [w.lemma_ for w in nlp(x)], min_df=10, max_df=.5)),\n",
    "                 ('nb', MultinomialNB())])\n",
    "pipe.fit(X_train, y_train)\n",
    "accuracy_score(y_test, pipe.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) [Tf-Idf](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "\n",
    "Tf-Idf is short for Term Frequency - Inverse Document Frequency and is a way of normalizing term counts. It is a product of two separate metrics:\n",
    "\n",
    "- _Term Frequency_ shows how often a word is appearing in a document. ${\\displaystyle \\mathrm {tf} (t,d)={\\frac {1}{2}} + {\\frac {f_{t,d}}{2 \\cdot \\max\\{f_{t',d}:t'\\in d\\}}}}$\n",
    "- _Inverse Document Frequency_ shows if a term is common or rare across all documents. $ \\mathrm {idf}(t, D) =  \\log \\frac{N}{|\\{d \\in D: t \\in d\\}|}$ where $N$ is the total number of documents in the corpus, $t$ is a term, $D$ is the set of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cntvec', CountVectorizer(analyzer=split_into_lemmas, min_df=5, max_df=.9)),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('nb', MultinomialNB())])\n",
    "pipe.fit(X_train, y_train)\n",
    "accuracy_score(y_test, pipe.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for word in np.argsort(pipe.steps[1][1].idf_)[-20:][::-1]:\n",
    "    print(word, pipe.steps[0][1].get_feature_names()[word], pipe.steps[1][1].idf_[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) [Hashing](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer)\n",
    "\n",
    "Really large corpora induce several problems with required memory: the larger the corpus, the larger the vocabulary will grow in memory.\n",
    "To avoid this issue a [_hashing trick_](http://scikit-learn.org/stable/modules/feature_extraction.html#feature-hashing) can be used. Basically instead of storing each different word in a dictionary it applies a hash function to the features to determine their column index in sample matrices directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('hash', HashingVectorizer(analyzer=split_into_lemmas, n_features=1000, non_negative=True)),\n",
    "                 ('nb', MultinomialNB())])\n",
    "pipe.fit(X_train, y_train)\n",
    "accuracy_score(y_test, pipe.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Latent Semantic Indexing\n",
    "\n",
    "_\"Latent semantic analysis (LSA) is a technique in natural language processing of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. LSA assumes that words that are close in meaning will occur in similar pieces of text.\"_ from: [wiki](https://en.wikipedia.org/wiki/Latent_semantic_analysis)  \n",
    "LSA can be created by appling [`SVD`](http://scikit-learn.org/stable/modules/decomposition.html#truncated-singular-value-decomposition-and-latent-semantic-analysis) to `Tf-Idf` vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cntvec', CountVectorizer(analyzer=split_into_lemmas, stop_words='english')),\n",
    "                 ('tfidf', TfidfTransformer(sublinear_tf=True)),\n",
    "                 ('svd', TruncatedSVD(n_components=100, random_state=42)),\n",
    "                 ('svm', SVC(C=300))])\n",
    "pipe.fit(X_train, y_train)\n",
    "accuracy_score(y_test, pipe.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feat_names = pipe.steps[0][1].get_feature_names()\n",
    "topics = pipe.steps[2][1].components_\n",
    "topic_str = pipe.steps[2][1].explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_most_important(topic, feat_names):\n",
    "    indices = np.argsort(topic)[-10:][::-1]\n",
    "    return pd.DataFrame([{'topic': feat_names[index], \n",
    "                          'weight': topic[index]}\n",
    "                         for index in indices]).set_index('topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=10, figsize=(15, 60))\n",
    "for i in range(10):\n",
    "    get_most_important(topics[i], feat_names).plot.bar(ax=ax[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Document similarity metrics\n",
    "\n",
    "The euclidean distance is not feasable to determine the likeness of documents. Instead the cosine similarity is used which is the angle between the document vectors.\n",
    "\n",
    "It is super convenient that the cosine similarity can be computed by the dot product between document tfidf vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(analyzer=split_into_lemmas,\n",
    "                        stop_words='english',\n",
    "                        min_df=10,\n",
    "                        max_df=.5).fit(corpus.message)\n",
    "vects = tfidf.transform(corpus.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vect = tfidf.transform([corpus.message[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sims = vects.dot(vect.T).toarray().flatten()\n",
    "most_similar = np.argsort(sims)[::-1][:10]\n",
    "\n",
    "print(corpus.message[0])\n",
    "print('-' * 80)\n",
    "for i, index in enumerate(most_similar):\n",
    "    print(i, sims[index])\n",
    "    print(corpus.message[index])\n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "- [nltk introduction](http://www.nltk.org/book/ch01.html)\n",
    "- [dive into nltk blogpost series](http://textminingonline.com/dive-into-nltk-part-i-getting-started-with-nltk)\n",
    "- [sklearn text data introduction](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)\n",
    "- [sklearn text feature extraction introduction](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "- [spacy introduction](https://spacy.io/usage/spacy-101)\n",
    "- [LSA tutorial](http://mccormickml.com/2016/03/25/lsa-for-text-classification-tutorial/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
