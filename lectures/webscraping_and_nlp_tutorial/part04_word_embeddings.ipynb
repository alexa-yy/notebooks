{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV. Word Embeddings\n",
    "\n",
    "---\n",
    "\n",
    "## What is word embedding?\n",
    "\n",
    "Word embedding is a method to map words into continous vectors. The generated vectors are the semantic representations of the words. There are a lot of different algorithms, the most commonly used is called `word2vec`.\n",
    "\n",
    "<img src=\"resources/w2v-context-words.png\">\n",
    "<img src=\"resources/w2v-king-queen-vectors.png\" align=\"left\" width=\"400px\">\n",
    "<img src=\"resources/w2v-king-queen-composition.png\" align=\"right\" width=\"400px\">\n",
    "\n",
    "<div style=\"clear:both\">\n",
    "    <small><a href=\"https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/\">Forrás</a></small>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## Word embeddings in practice\n",
    "\n",
    "One of the most accessable library which can be used to generate word2vec - a type of werd embedding - is `gensim`. The word2vec algorithm requires preprocessed sentences in order to train.\n",
    "\n",
    "### 1. Corpus preparation\n",
    "\n",
    "- Acquire the data\n",
    "- Split into sentences\n",
    "- Tokenize the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import spacy\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "tokenizer = PunktSentenceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences):\n",
    "    return [[token.lemma_ for token in nlp(sent) \n",
    "             if not token.is_stop\n",
    "             and not token.is_punct\n",
    "             and not token.is_space\n",
    "             and not token.lemma_ == '-PRON-']\n",
    "            for sent in tqdm.tqdm(sentences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOTR = {}\n",
    "for i in range(3):\n",
    "    url ='http://ae-lib.org.ua/texts-c/tolkien__the_lord_of_the_rings_{book}__en.htm'\n",
    "    resp = requests.get(url.format(book=i+1)).content\n",
    "    text = BeautifulSoup(resp, \"html.parser\").getText()\n",
    "    sentences = tokenizer.tokenize(text)\n",
    "    LOTR[i] = tokenize(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training\n",
    "Now that we transformed the raw data to the desired format we can train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(LOTR[0] + LOTR[1] + LOTR[2], \n",
    "                               size=50, window=5, min_count=5, iter=5, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using the trained model\n",
    "The trained model can be used to:\n",
    "- search for similar items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar('hobbit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- search for analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=['sméagol', 'hobbit'], negative=['ring'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = model['sméagol'] - model['ring'] + model['hobbit']\n",
    "model.wv.most_similar([vector])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- find which word from the given list doesn't go with the others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.doesnt_match(['gandalf', 'frodo', 'saruman', 'aragorn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- visualize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ringwords = ['ring'] + [w for w, s in model.most_similar(['ring'], topn=20)]\n",
    "ring = np.array([word in ringwords for word in model.wv.vocab.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model[model.wv.vocab]\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.scatter(X_tsne[~ring][:, 0], X_tsne[~ring][:, 1], c='b', alpha=.3)\n",
    "ax.scatter(X_tsne[ring][:, 0], X_tsne[ring][:, 1], c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Saving models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('data/lotr_w2v.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Loading pre-trained models\n",
    "- gensim models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load('data/lotr_w2v.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- other, binary models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set binary to true if the model is compressed\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz', binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar('Frodo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "- [gensim word2vec tutorial](https://rare-technologies.com/word2vec-tutorial/)\n",
    "- [demistifying word2vec](http://www.deeplearningweekly.com/blog/demystifying-word2vec)\n",
    "- [word2vec by hand blogpost](https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/)\n",
    "- [visualizing embeddings](https://github.com/anvaka/word2vec-graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
