{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV. Word Embeddings\n",
    "\n",
    "---\n",
    "\n",
    "## What is word embedding?\n",
    "\n",
    "Word embedding is a method to map words into continous vectors. The generated vectors are the semantic representations of the words. There are a lot of different algorithms, the most commonly used is called `word2vec`.\n",
    "\n",
    "<img src=\"resources/w2v-context-words.png\">\n",
    "<img src=\"resources/w2v-king-queen-vectors.png\" align=\"left\" width=\"400px\">\n",
    "<img src=\"resources/w2v-king-queen-composition.png\" align=\"right\" width=\"400px\">\n",
    "\n",
    "<div style=\"clear:both\">\n",
    "    <small><a href=\"https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/\">Forr√°s</a></small>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## Word embeddings in practice\n",
    "\n",
    "One of the most accessable library which can be used to generate word2vec - a type of werd embedding - is `gensim`. The word2vec algorithm requires preprocessed sentences in order to train.\n",
    "\n",
    "### 1. Corpus preparation\n",
    "\n",
    "- Acquire the data\n",
    "- Split into sentences\n",
    "- Tokenize the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import spacy\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "tokenizer = PunktSentenceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences):\n",
    "    return [[token.lemma_ for token in nlp(sent) \n",
    "             if not token.is_stop\n",
    "             and not token.is_punct\n",
    "             and not token.is_space\n",
    "             and not token.lemma_ == '-PRON-']\n",
    "            for sent in tqdm.tqdm(sentences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOTR = {}\n",
    "for i in range(3):\n",
    "    url ='http://ae-lib.org.ua/texts-c/tolkien__the_lord_of_the_rings_{book}__en.htm'\n",
    "    resp = requests.get(url.format(book=i+1)).content\n",
    "    text = BeautifulSoup(resp, \"html.parser\").getText()\n",
    "    sentences = tokenizer.tokenize(text)\n",
    "    LOTR[i] = tokenize(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(LOTR[0] + LOTR[1] + LOTR[2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar('hobbit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=['frodo', 'dwarf'], negative=['hobbit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Saving models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('data/lotr_w2v.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Loading pre-trained models\n",
    "- gensim models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load('data/lotr_w2v.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- other, binary models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set binary to true if the model is compressed\n",
    "model = gensim.models.Word2Vec.load_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz', binary=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Using semantic vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "- [gensim word2vec tutorial](https://rare-technologies.com/word2vec-tutorial/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
