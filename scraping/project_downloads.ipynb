{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import time\n",
    "import pathlib\n",
    "from urllib.parse import unquote\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from requests_futures.sessions import FuturesSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLOBAL OPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKER_THREADS = 50\n",
    "MIN_WORKER_THREADS = 5\n",
    "CHECK_INTERVAL = 120\n",
    "STALLED_LIMIT = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## INIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 'https://emir.palyazat.gov.hu/nyertes/'\n",
    "url = 'https://emir.palyazat.gov.hu/nyertes/index.php'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    'node': 'export',\n",
    "    'forras': '1420',      # SZ2020\n",
    "    'op_type': 'op_nev',   # op_type\n",
    "    'op_nev': '1382',      # OP program\n",
    "    'eupik_nev': '180100', # tamogatasi konstrukcio\n",
    "    'palyazo_nev': '',\n",
    "    'regio': '0',\n",
    "    'megye': '0',\n",
    "    'kisterseg': '0',\n",
    "    'helyseg': '0',\n",
    "    'ttipus': '',\n",
    "    'tkod': '',\n",
    "    'ttype': '',\n",
    "    'print': '0',\n",
    "    'export': '1',\n",
    "    'id_szerv': '0',\n",
    "    'sort': 'asc',\n",
    "    'order': 'NEV',\n",
    "    'page': '1',\n",
    "    'rows': '10000',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = pd.read_excel('./data/CODEMAPPING.xlsx')\n",
    "codes.columns = ['source_code', 'source_name', 'op_type', 'op_code', \n",
    "                 'op_name', 'eupik_code', 'eupik_name']\n",
    "\n",
    "codes['source_code'] = codes['source_code'].astype(str)\n",
    "codes['op_code'] = codes['op_code'].astype(str)\n",
    "codes['op_type'] = codes['op_type'].astype(str)\n",
    "codes['eupik_code'] = codes['eupik_code'].astype(str)\n",
    "\n",
    "codes['index'] = codes.source_code.str.cat([codes.op_code, codes.eupik_code], '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query(query, source_code, op_code, op_type, eupik_code):\n",
    "    filled = query.copy()\n",
    "    filled['forras'] = source_code\n",
    "    filled['op_nev'] = op_code\n",
    "    filled['op_type'] = op_type\n",
    "    filled['eupik_nev'] = eupik_code\n",
    "    return filled\n",
    "\n",
    "\n",
    "def download_one(session, index, row):\n",
    "    params = generate_query(query, row.source_code, row.op_code, row.op_type, row.eupik_code)\n",
    "    return {\n",
    "        'index': index,\n",
    "        'resp': session.get(url, params=params, verify=False, \n",
    "                            background_callback=export_result)\n",
    "    }\n",
    "\n",
    "\n",
    "def download_all(session, codes):\n",
    "    return [download_one(session, index, row)\n",
    "            for index, row in codes.iterrows()]\n",
    "\n",
    "\n",
    "def check_status(responses, stalled_list):\n",
    "    numfinished = sum([resp['resp'].done() for resp in responses])\n",
    "    numrunning = sum([resp['resp'].running() for resp in responses])\n",
    "    for response in responses:\n",
    "        index = response['index']\n",
    "        resp = response['resp']\n",
    "        if resp.done():\n",
    "            stalled_list = [download for download in stalled_list \n",
    "                            if not download == index]\n",
    "        elif resp.running():\n",
    "            stalled_list.append(index)\n",
    "    \n",
    "    return numfinished, numrunning, stalled_list\n",
    "\n",
    "\n",
    "def restart_download(session, codes, download):\n",
    "    index = download['index']\n",
    "    row = codes.loc[download['index']]\n",
    "    download['resp'].cancel()\n",
    "    return download_one(session, index, row)\n",
    "\n",
    "\n",
    "def handle_stalled(responses, stalled_list, session, codes):\n",
    "    targets = [download for download, cnt in Counter(stalled_list).items() \n",
    "               if cnt > STALLED_LIMIT - 1]\n",
    "    stalled_list = [download for download in stalled_list \n",
    "                    if download not in targets]\n",
    "    print('Handling stalled downloads: {}'.format(len(targets)))\n",
    "\n",
    "    responses = [restart_download(session, codes, response)\n",
    "                 if response['index'] in targets \n",
    "                 else response\n",
    "                 for response in responses]\n",
    "                    \n",
    "    return responses, stalled_list\n",
    "\n",
    "\n",
    "def export_result(sess, resp):\n",
    "    query_param = dict([p.split('=') \n",
    "                        for p in resp.request.path_url.split('&') \n",
    "                        if p.startswith('forras') \n",
    "                        or p.startswith('op_nev') \n",
    "                        or p.startswith('op_type') \n",
    "                        or p.startswith('eupik_nev')])\n",
    "    df = pd.read_csv(io.StringIO(resp.content.decode('latin1')), sep=';')\n",
    "    \n",
    "    \n",
    "    source_code = query_param['forras']\n",
    "    op_code = query_param['op_nev']\n",
    "    op_type = query_param['op_type']\n",
    "    eupik_code = query_param['eupik_nev']\n",
    "    \n",
    "    if len(df) > 0:\n",
    "        df['source_code'] = str(source_code)\n",
    "        df['op_code'] = str(unquote(op_code))\n",
    "        df['op_type'] = str(op_type)\n",
    "        df['eupik_code'] = str(eupik_code)\n",
    "\n",
    "        dirname = f'./data/projects/{source_code}/{op_code}'\n",
    "        if not os.path.exists(dirname):\n",
    "            os.makedirs(dirname)\n",
    "        filename = dirname + f'/{eupik_code}.csv'\n",
    "\n",
    "        df.to_csv(filename, index=False)\n",
    "    else:\n",
    "        with open('./errors.txt', 'a') as f:\n",
    "            f.write(f'{source_code}_{op_type}_{unquote(op_code)}_{eupik_code}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## DOWNLOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_finished(codes, data_dir):\n",
    "    print('Filtering finished downloads from codes df... ', end='')\n",
    "    files = [{'index': '{}_{}_{}'.format(source_code, str(unquote(op_code)), eupik_code[:-4]),\n",
    "              'source_code': str(source_code),\n",
    "              'op_code': str(unquote(op_code)),\n",
    "              'eupik_code': str(eupik_code[:-4]),\n",
    "              'df': pd.read_csv(f'{data_dir}{source_code}/{op_code}/{eupik_code}')}\n",
    "         for source_code in os.listdir(data_dir)\n",
    "         for op_code in os.listdir(data_dir + source_code)\n",
    "         for eupik_code in os.listdir(data_dir + source_code + '/' + op_code)]\n",
    "    \n",
    "    dfs = []\n",
    "    for data in files:\n",
    "        df = data['df']\n",
    "        df.columns = [col.strip() for col in df.columns]\n",
    "        df['source_code'] = str(data['source_code'])\n",
    "        df['op_code'] = str(data['op_code'])\n",
    "        df['eupik_code'] = str(data['eupik_code'])\n",
    "        df['index'] = data['index']\n",
    "        df['done'] = True\n",
    "        dfs.append(df)\n",
    "        \n",
    "    merged = pd.merge(left=codes, right=pd.concat(dfs, ignore_index=True), \n",
    "                      on='index', how='left')\n",
    "        \n",
    "    indices = merged.loc[~merged.done.fillna(False), 'index'].unique()\n",
    "    print('done.')\n",
    "    return codes.loc[codes['index'].isin(indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start(codes):\n",
    "    print('session init... ', end='')\n",
    "    session = requests.session()\n",
    "    _ = session.get(base, verify=False)\n",
    "    session = FuturesSession(session=session, \n",
    "                             executor=ThreadPoolExecutor(max_workers=WORKER_THREADS))\n",
    "    print('done. starting downloads... ', end='')\n",
    "    responses = download_all(session, codes)\n",
    "    print('started.')\n",
    "    return session, responses\n",
    "\n",
    "\n",
    "def restart(session, codes, responses, data_dir):\n",
    "    print('shutting down running downloads... ', end='')\n",
    "    for response in responses:\n",
    "        response['resp'].cancel()\n",
    "    session.close()\n",
    "    print('shutdown complete, starting new session...')\n",
    "    return start(filter_finished(codes, data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session, responses = start(filter_finished(codes, './data/projects/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finished = False\n",
    "stalled = []\n",
    "while not finished:\n",
    "    numfinished, numrunning, stalled = check_status(responses, stalled)\n",
    "    if numfinished == len(responses):\n",
    "        finished = True\n",
    "        continue\n",
    "\n",
    "    responses, stalled = handle_stalled(responses, stalled, session, codes)\n",
    "    \n",
    "    print(f'{numfinished / len(responses) * 100:.2f}% done. '\n",
    "          f'[{numfinished}/{len(responses)}] '\n",
    "          f'- {numrunning} active downloads')\n",
    "    \n",
    "    if numrunning < min(len(responses) - numfinished, MIN_WORKER_THREADS):\n",
    "        session, responses = restart(session, codes, responses, './data/projects/')\n",
    "    \n",
    "    time.sleep(CHECK_INTERVAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from urllib.parse import unquote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/'\n",
    "files = [{'index': '{}_{}_{}'.format(source_code, str(unquote(op_code)), eupik_code[:-4]),\n",
    "          'source_code': str(source_code),\n",
    "          'op_code': str(unquote(op_code)),\n",
    "          'eupik_code': str(eupik_code[:-4]),\n",
    "          'df': pd.read_csv(f'{data_dir}{source_code}/{op_code}/{eupik_code}')}\n",
    "         for source_code in os.listdir(data_dir)\n",
    "         for op_code in os.listdir(data_dir + source_code)\n",
    "         for eupik_code in os.listdir(data_dir + source_code + '/' + op_code)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = pd.read_excel('./CODEMAPPING.xlsx')\n",
    "codes.columns = ['source_code', 'source_name', 'op_type', 'op_code', \n",
    "                 'op_name', 'eupik_code', 'eupik_name']\n",
    "\n",
    "codes['source_code'] = codes['source_code'].astype(str)\n",
    "codes['op_code'] = codes['op_code'].astype(str)\n",
    "codes['eupik_code'] = codes['eupik_code'].astype(str)\n",
    "\n",
    "codes['index'] = codes.source_code.str.cat([codes.op_code, codes.eupik_code], '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for data in files:\n",
    "    df = data['df']\n",
    "    df.columns = [col.strip() for col in df.columns]\n",
    "    df['source_code'] = str(data['source_code'])\n",
    "    df['op_code'] = str(data['op_code'])\n",
    "    df['eupik_code'] = str(data['eupik_code'])\n",
    "#    df['index'] = data['index']\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(left=codes, right=pd.concat(dfs, ignore_index=True), \n",
    "                  on=['source_code', 'op_code', 'eupik_code'], how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv(data_dir + 'merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_excel(data_dir + 'merged.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
