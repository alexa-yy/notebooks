{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Data Science\n",
    "## Part III. - Data Transformation\n",
    "\n",
    "### Table of contents\n",
    "- <a href=\"#What-is-Data-Transformation?\">Theory</a>\n",
    "- <a href=\"#1.-Numerical-features\">Numerical features</a>\n",
    "- <a href=\"#2.-Nominal-features\">Nominal features</a>\n",
    "- <a href=\"#3.-FeatureUnions\">Feature Unions</a>\n",
    "\n",
    "## What is Data Transformation?\n",
    "During data transformation the goal is to prepare the data to be usable in the modelling steps. These transformations include normalization, standardization, text processing, generating complex features from basic ones, or any kind of data mapping.\n",
    "\n",
    "_\"...a data transformation converts a set of data values from the data format of a source data system into the data format of a destination data system._\n",
    "\n",
    "_Data transformation can be divided into two steps:_\n",
    "1. _data mapping maps data elements from the source data system to the destination data system and captures any transformation that must occur_\n",
    "2. _code generation that creates the actual transformation program\"_\n",
    "from: <a href=\"https://en.wikipedia.org/wiki/Data_transformation\">Wikipedia</a>\n",
    "\n",
    "### Why is it important?\n",
    "\n",
    "Most of the models are sensitive to data, so you must transform it into a more desired format. Unfortunately the data you start with is usually in terrible shape:\n",
    "\n",
    "- It has missing values\n",
    "- It is full of outliers\n",
    "- The data is distorted by noise\n",
    "- The features are in different scales\n",
    "- The features are correlated/redundant/uninformative\n",
    "\n",
    "\n",
    "### Tools\n",
    "\n",
    "- scaling/binarizing\n",
    "- normalizing/standardizing\n",
    "- outlier detecting\n",
    "- filtering\n",
    "- mathematical transformations\n",
    "- representational changes\n",
    "- etc.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermission - prototype based classifiers\n",
    "\n",
    "### K-Nearest Neighbour classification\n",
    "If it looks like a duck, and quacks like a duck is probably a duck. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the loan dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/loan.csv', index_col=0)\n",
    "\n",
    "numerical_cols = ['ApplicantIncome', 'CoapplicantIncome', 'Dependents',\n",
    "                  'LoanAmount', 'Loan_Amount_Term']\n",
    "nominal_cols = ['Loan_ID', 'Gender', 'Married', 'Education',\n",
    "                'Credit_History', 'Self_Employed', 'Property_Area']\n",
    "target_col = ['Target']\n",
    "\n",
    "X = df[numerical_cols + nominal_cols]\n",
    "y = df[target_col]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3., random_state=41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Numerical features\n",
    "\n",
    "\n",
    "### <a href=\"http://pandas.pydata.org/pandas-docs/stable/missing_data.html\">missing values</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing = pd.DataFrame()\n",
    "missing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dropping NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dropped = missing.dropna(axis=0)\n",
    "dropped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fill NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filled = missing.fillna(value=0)\n",
    "filled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- intepolate NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "interpolated = missing.interpolate(method='nearest')\n",
    "interpolated.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"http://pandas.pydata.org/pandas-docs/stable/missing_data.html#values-considered-missing\">infinite values</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('mode.use_inf_as_null', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.finfo('d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dropped = missing.dropna(axis=0)\n",
    "dropped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filled = missing.fillna(value=0)\n",
    "filled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "interpolated = missing.interpolate()\n",
    "interpolated.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"http://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling\">different scales</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaled[[6]] = scaler.fit_transform(scaled[[6]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: move somewhere where it makes at least minimal sense\n",
    "def gridplot(X, y=None):\n",
    "    g = sns.PairGrid(X, hue=y)\n",
    "    g = g.map_diag(plt.hist)\n",
    "    g = g.map_offdiag(plt.scatter)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: transform y_train to numerical value to color the dots by classes\n",
    "gridplot(X_train[numerical_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"http://scikit-learn.org/stable/modules/preprocessing.html#normalization\">unnormalized data</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(), robust=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not now. More about this topic in the next issue of DS101. Cough-cough-<a href=\"http://scikit-learn.org/stable/modules/preprocessing.html#scaling-data-with-outliers\" style=\"color: black; text-decoration: none; cursor: default;\">PCA</a>-cough.\n",
    "\n",
    "### <a href=\"http://scikit-learn.org/stable/modules/preprocessing.html#scaling-data-with-outliers\">outliers</a>\n",
    "\n",
    "<img src=\"https://i0.wp.com/flowingdata.com/wp-content/uploads/2014/09/outlier.gif\" align=\"left\" width=\"400\">\n",
    "\n",
    "<br style=\"clear:left;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"http://scikit-learn.org/stable/modules/preprocessing.html#feature-binarization\">binarization</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "binarizer = Binarizer()\n",
    "binarizer.fit_transform(df)[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Nominal Features\n",
    "\n",
    "### Replacing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "cleanup_nums = {\"num_doors\":     {\"four\": 4, \"two\": 2},\n",
    "                \"num_cylinders\": {\"four\": 4, \"six\": 6, \"five\": 5, \"eight\": 8,\n",
    "                                  \"two\": 2, \"twelve\": 12, \"three\":3 }}\n",
    "obj_df.replace(cleanup_nums, inplace=True)\n",
    "obj_df.head()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"\">Label encoding</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"http://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features\">One-hot encoding</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "__\\# TODO:__ replace outdated cells \n",
    "```python\n",
    "categorical = binarize(data)\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "encoder.fit_transform(categorical).todense()\n",
    "\n",
    "categorical = np.array([[np.random.choice([0,1,2])]\n",
    "                        for _ in range(100)])\n",
    "\n",
    "encoded = encoder.fit_transform(categorical)\n",
    "encoded.shape\n",
    "\n",
    "categorical[:15]\n",
    "\n",
    "encoded[:15].todense()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label binarizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FeatureUnions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    # TODO: add prepocessed features \n",
    "    ('knn', KNeighborsClassifier())    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use binary metric instead of rmse?\n",
    "baseline = np.ones((len(y_train), 1)) * y_train.mean()\n",
    "mean_squared_error(y_train, baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)\n",
    "y_hat = pipe.predict(X_test)\n",
    "mean_squared_error(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model of the week: Decision trees\n",
    "\n",
    "Decision trees are a type of supervised machine learning algorithms that can be used to predict both categorical and continuous values (in this case they are called *regression trees*). **Basically the algorithm divides the training population along the values of their attributes, and assigns a prediction value to each of these categories.** In the case of **categorical** target variable, the assigned prediction will be **the mode of the target variable values** in the particular subpopulation. In the other case it will be **the mean of the values**.  \n",
    "An example which will be familiar, from the <a href=\"https://en.wikipedia.org/wiki/Decision_tree_learning\">wikipedia page of decision trees</a>:  \n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png\">  \n",
    "A tree showing survival of passengers on the Titanic (\"sibsp\" is the number of spouses or siblings aboard). The figures under the leaves show the probability of outcome and the percentage of observations in the leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, where's the trick?  \n",
    "At deciding *where to split the population*. **The goal is to make these subpopulations as homogenous in the target variable as we can.** (So naturally if the target variable and other variables are completely uncorrelated with an attribute, the decision tree won't split the population according to that variable.) There are multiple *metrics* used to decide what splitting is good at a particular point in the tree, but most of them calculate an **\"impurity\"** of the parent node, and choose a split that will furthest reduce this impurity, most of the time only considering the local node - meaning **it doesn't ensure that the final output will be the best globally**.  \n",
    "\n",
    "#### How big/complex tree do we want to build? Or: When should a node be declared a leaf?\n",
    "As one of the biggest weakness of decision/regression trees is **overfitting**, these are very important questions. There are multiple ways to limit a tree: \n",
    "- Set a minimum sample number at which a split can be made\n",
    "- Set a maximum sample number at which a node can be a leaf\n",
    "- Set a threshold for the impurity decrease, below which no splits are made\n",
    "- Set a maximum depth of the tree\n",
    "- Set the maximum number of variables used for splitting\n",
    "- Set a maximum number of leaves\n",
    "\n",
    "Another answer to overfitting is **tree pruning**. This basically means to let the tree grow to the point where every leaf has only a few number of observations, and then starting from the top or bottom, get rid of the splits that don't contribute too much to the accuraccy of the model.\n",
    "\n",
    "### And why are trees better than say, logistic regression?\n",
    "The most important case is when **there is a non-linear relation between the attributes and the target variable**. Another good side of decision trees is that despite this they are very easy to interpret (just look at the picture above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### What is better than a tree? Multiple trees!\n",
    "To tackle overfitting, an early technique was to randomly sample the training set and using different samples, build multiple trees. Then the prediction is made using the \"votes\" of the trees (either the mean or the mode of them). This is called **bagging**. **Random forests** enhance this procedure by excluding random attributes from the potential splitting at each node. The reasoning behind this is that if there are a few attributes which have very good explaining powers, they would be used at most of the splits - and so the different trees would be strongly correlated, reducing the effect of bagging.  \n",
    "An important thing to remember:  \n",
    "> When in doubt, use <s>brute force</s> random forest."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
