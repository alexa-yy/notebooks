{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Data Science\n",
    "## Part V. Text Mining\n",
    "\n",
    "### Table of contents\n",
    "- #### Text Mining\n",
    "    - <a href=\"#What-is-Text-mining?\">Theory</a>\n",
    "    - <a href=\"#Tools\">In practice</a>\n",
    "- #### Neural Networks\n",
    "    - <a href=\"#Neural-Networks\">Theory</a>\n",
    "    - <a href=\"#Example\">Linear regression</a>\n",
    "    \n",
    "\n",
    "## What is Text Mining?\n",
    "Text mining or text analytics is the process of extracting quantified features from (un)structured (natural language) texts. Processing unstructured data involves using Natural Language Processing (NLP), statistical modeling and machine learning techniques.\n",
    "\n",
    "## Why is it important?\n",
    "80% of the generated data is not available in structured, numerical format (emails, texts, meeting notes, documents, social media feeds). These unstructured data includes images, drawings, videos, voice recordings and unstructured texts. These data can be described with their meta data (length, topic, category, etc.) but transforming them into structured data is important to access the more detailed information stored in such data sources. Voice recordings, videos and drawings can also be transcribed into unstructured texts so they can be processing as textual data. \n",
    "Most common use cases are:\n",
    "\n",
    "- document similarity computation\n",
    "- document deduplication\n",
    "- document clustering\n",
    "- topic extraction\n",
    "- sentiment analysis\n",
    "- automated annotation\n",
    "- text filtering\n",
    "- text classification\n",
    "\n",
    "## Tools\n",
    "- NLP tools\n",
    "    - tokenization\n",
    "    - stemming\n",
    "    - part-of-speech (POS) tagging\n",
    "    - stop word filtering\n",
    "    - bag-of-words representation\n",
    "    - tf-idf transformation\n",
    "- Other tools\n",
    "    - Word2Vec representation\n",
    "    - hashing\n",
    "    - cosine/jaccard/levenshtein/etc similarity computation\n",
    "    - matrix factorization\n",
    "    - etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Mining in practice\n",
    "\n",
    "### 1. Read and examine data\n",
    "\n",
    "Examining unstructured data is the key to proper preprocessing.  \n",
    "The collection of texts is called __`corpus`__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./data/SMSSpamCollection') as spamfile:\n",
    "    corpus = [line.strip() for line in spamfile]\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for text in corpus[:5]:\n",
    "    print text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the data is in TSV format, read it accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = pd.read_csv('./data/SMSSpamCollection', sep='\\t', names=['label', 'message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus['length'] = corpus.message.str.len()\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus.length.plot(bins=20, kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus.length.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "910 long sms???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(corpus.message[corpus.length > 900])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there a difference between spam and ham messages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus.hist(bins=50, by='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why not try a simple predictor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splitted = train_test_split(corpus.length.values[:, np.newaxis], # we need a matrix, not a vector\n",
    "                            corpus.label.values,\n",
    "                            train_size=.75,\n",
    "                            random_state=42)\n",
    "X_train, X_test, y_train, y_test = splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('nb', MultinomialNB())])\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, pipe.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "87% percent is our baseline, let's get into the preprocessing!\n",
    "\n",
    "### 2. Preprocessing\n",
    "#### a) Bag-of-words representation\n",
    "\n",
    "Bag of words representation represent documents as a vector where each different word is represented in a fixed position. The values in the positions are the occurence counts in the given document. For example:\n",
    "The vector features for the documents:\n",
    "```python\n",
    "docs = [\"I like trains.\", \"Trains are like big cars.\", \"I like big cars\"]\n",
    "```\n",
    "will be \n",
    "```python\n",
    "features = {'I': 0, 'like': 1, 'trains': 2, 'are': 3, 'big': 4, 'cars': 5}\n",
    "```\n",
    "and the vectorial representations will be\n",
    "```python\n",
    "vectors = [[1, 1, 1, 0, 0, 0],\n",
    "           [0, 1, 1, 1, 1, 1],\n",
    "           [1, 1, 0, 0, 1, 1]]\n",
    "```\n",
    "\n",
    "Fortunately `scikit-learn` has a built-in solution for this: the [`CountVectorizer`](http://scikit-learn.org/stable/modules/feature_extraction.html#the-bag-of-words-representation).  \n",
    "Let's try out our little example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cntvec = CountVectorizer()\n",
    "docs = [\"I like trains.\",\n",
    "        \"Trains are like big cars.\",\n",
    "        \"I like big cars\"]\n",
    "cntvec.fit_transform(docs).todense(), cntvec.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-grams\n",
    "N-grams are n long word tuples. They are generated by an n long rolling window and they can provide contextual information which sometimes yields better results. An example 2-gram for the `\"I like trains.\"` sentence would be:\n",
    "```python\n",
    "[(\"I\", \"like\"), (\"like\", \"trains\")]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cntvec = CountVectorizer(ngram_range=(2, 2))\n",
    "cntvec.fit_transform(docs).todense(), cntvec.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum and maximum document frequency\n",
    "\n",
    "Minimum and maximum document frequency (`min_df` and `max_df`) are set thresholds to limit the feature numbers. If a _term_ (transformed word) appears less than `min_df` or more than `max_df` times (or percent) then it will be left out from the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cntvec = CountVectorizer(max_df=1)\n",
    "cntvec.fit_transform(docs).todense(), cntvec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cntvec = CountVectorizer(min_df=3)\n",
    "cntvec.fit_transform(docs).todense(), cntvec.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "\n",
    "In the vocabulary generation process each word are analyzed and transformed in order to reduce vocabulary length. Scikit-learn's default analization function is lowercasing the words and filtering short and stop words but no further transformation is done.\n",
    "\n",
    "NLP has more detailed techniques to better extract the base words. Lemmatization is a powerful tool to reduce a word into it's _root_ form (as it appears in dictionaries): that's how `are` becomes `be` and `trains` becomes `train`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_into_lemmas(message):\n",
    "    message = unicode(message, 'utf8').lower()\n",
    "    words = TextBlob(message).words\n",
    "    return [word.lemma for word in words]\n",
    "\n",
    "[split_into_lemmas(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cntvec = CountVectorizer(analyzer=split_into_lemmas)\n",
    "cntvec.fit_transform(docs).todense(), cntvec.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's insert our vectorizer to our pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splitted = train_test_split(corpus.message,\n",
    "                            corpus.label.values,\n",
    "                            train_size=.75,\n",
    "                            random_state=42)\n",
    "X_train, X_test, y_train, y_test = splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cntvec', CountVectorizer(analyzer=split_into_lemmas, min_df=10, max_df=.5)),\n",
    "                 ('nb', MultinomialNB())])\n",
    "pipe.fit(X_train, y_train)\n",
    "accuracy_score(y_test, pipe.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(pipe.steps[0][-1].vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Tf-Idf\n",
    "\n",
    "\\# TODO!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cntvec', CountVectorizer(analyzer=split_into_lemmas, min_df=5, max_df=.9)),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('nb', MultinomialNB())])\n",
    "pipe.fit(X_train, y_train)\n",
    "accuracy_score(y_test, pipe.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for word in np.argsort(pipe.steps[1][1].idf_)[-20:][::-1]:\n",
    "    print word, pipe.steps[0][1].get_feature_names()[word], pipe.steps[1][1].idf_[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Hashing\n",
    "\n",
    "\\# TODO!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('hash', HashingVectorizer(analyzer=split_into_lemmas, n_features=1000, non_negative=True)),\n",
    "                 ('nb', MultinomialNB())])\n",
    "pipe.fit(X_train, y_train)\n",
    "accuracy_score(y_test, pipe.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Latent Semantic Indexing\n",
    "\n",
    "Topic modelling for the masses. Tfidf+SVD=LSA\n",
    "\n",
    "\\# TODO!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cntvec', CountVectorizer(analyzer=split_into_lemmas)),\n",
    "                 ('tfidf', TfidfTransformer(sublinear_tf=True)),\n",
    "                 ('svd', TruncatedSVD(n_components=300, random_state=42)),\n",
    "                 ('svm', SVC(C=300))])\n",
    "pipe.fit(X_train, y_train)\n",
    "accuracy_score(y_test, pipe.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model of the week:\n",
    "### Neural Networks\n",
    "They are fun!\n",
    "\n",
    "\\# TODO!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "XOR_X, XOR_y = np.array([[0,0], [0, 1], [1, 0], [1, 1]]), np.array([0, 1, 1, 0])\n",
    "df = pd.DataFrame(data=XOR_X, columns=['x', 'y'])\n",
    "df['label'] = XOR_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_results_with_hyperplane(clf, clf_name, df, ax):\n",
    "    x_min, x_max = df.x.min() - .5, df.x.max() + .5\n",
    "    y_min, y_max = df.y.min() - .5, df.y.max() + .5\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, .02), np.arange(y_min, y_max, .02))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    ax.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "    ax.scatter(df.x, df.y, c=df.label, edgecolors='k')\n",
    "    ax.set_title(clf_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "perceptron = Perceptron(verbose=2, random_state=42).fit(XOR_X, XOR_y)\n",
    "perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_results_with_hyperplane(perceptron, 'perceptron', df, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(XOR_y, perceptron.predict(XOR_X))\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.heatmap(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(random_state=42).fit(XOR_X, XOR_y)\n",
    "mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_results_with_hyperplane(mlp, 'mlp', df, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(XOR_y, mlp.predict(XOR_X))\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
