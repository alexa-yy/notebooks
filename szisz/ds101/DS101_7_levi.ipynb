{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "Regression - just as classification - is a supervised machine learning problem however in case of regression the target variable is continuous. It is also \"a statistical process for estimating the relationships among variables. It includes many techniques for modeling and analyzing several variables, when the focus is on the relationship between a dependent variable and one or more independent variables (or 'predictors').\" from: <a href=\"https://en.wikipedia.org/wiki/Regression_analysis\">Wiki</a>\n",
    "\n",
    "It is important to note that instead of the descriptive nature of statistical regression analysis Data Science focuses on the predictive side of this method.\n",
    "\n",
    "## Why is it important?\n",
    "_\"Regression analysis is widely used for prediction and forecasting, where its use has substantial overlap with the field of machine learning.\"_ from: <a href=\"https://en.wikipedia.org/wiki/Regression_analysis\">Wiki</a>\n",
    "\n",
    "It is used to forecast any continuous variable:\n",
    "- stock market\n",
    "- salary prediction\n",
    "- network traffic\n",
    "- traffic\n",
    "- etc.\n",
    "\n",
    "## Tools\n",
    "- Linear regression\n",
    "- Ridge regression\n",
    "- LASSO\n",
    "- Bayesian regression\n",
    "- Support Vector regression\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\bs}[1]{\\boldsymbol{#1}}$\n",
    "\n",
    "## Variations on a Theme\n",
    "\n",
    "The traditional linear problem is stated like this:\n",
    "$$ y_i = \\bs{x}_i \\bs{\\beta} $$\n",
    "for every observation $i$, or more compactly\n",
    "$$ \\bs{y} = \\bs{X}\\bs{\\beta} $$\n",
    "where $ \\bs{X} $ is the matrix observed values, $\\bs{y}$ is the vector of observed output variables, and $\\bs{\\beta}$ is the weight vector which we want to find. \n",
    "\n",
    "In OLS, we try to find the $\\bs{\\beta}$ while minimizing a *loss function*, which is simply the sum of squares of the differences between the predicted and observed values (also called sum of squared residuals or SSR), \n",
    "\n",
    "$ \\mathrm{Cost}(\\bs{\\beta}) = \\mathrm{SSR}(\\bs{\\beta}) = \\sum _i (\\hat y_i - y_i)^{2} $.  \n",
    "\n",
    "<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\">Ridge</a>, <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\">LASSO</a> and <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge.html\">Bayesian</a> regressions (and a couple more) are basically simple <a href\"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\">linear</a> regressions, but with the loss function being modified.  \n",
    "Ridge regression adds the sum of the squares of the weights with a constant multiplier to the loss, i.e.\n",
    "\n",
    "$ \\mathrm{Cost}(\\bs{\\beta}) = \\sum _i (\\hat y_i - y_i)^{2} + \\alpha \\sum _i \\beta _i^{2}. $\n",
    "\n",
    "LASSO adds the sum of the absolute values of the coefficients, i.e.\n",
    "\n",
    "$ \\mathrm{Cost}(\\bs{\\beta}) = \\sum _i (\\hat y_i - y_i)^{2} + \\alpha \\sum _i \\vert \\beta _i. \\vert $\n",
    "\n",
    "### Ok, but what is the point of this?\n",
    "\n",
    "This technique is called **regularization**, and the use of this in our case is to prevent the model from **overfitting** to the data (which is our greatest enemy, right before **the curse of dimensionality**). Basically it prevents the coefficients from growing too large. To illustrate this, we will use the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html\">*boston dataset*</a>. (You should also check out <a href=\"https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/\">this</a> for a more detailed discussion on Ridge and LASSO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "lsop_train = X_train[:,12][..., np.newaxis]\n",
    "lsop_test = X_test[:,12][..., np.newaxis]\n",
    "curve_x = np.linspace(-10,50, num=300)[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(lsop_train, y_train)\n",
    "plt.xlabel(\"% lower status of the population\")\n",
    "plt.ylabel(\"Median value of owner-occupied homes in $1000's\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ols = Pipeline([('poly', PolynomialFeatures()), ('ols', LinearRegression())])\n",
    "parameters = {'poly__degree': range(1,16)}\n",
    "grid_search = GridSearchCV(ols, parameters, n_jobs=2, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_search.fit(lsop_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print grid_search.best_params_\n",
    "print grid_search.best_estimator_.score(lsop_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For plotting purposes, explicitly create the pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipes = {}\n",
    "for degree in range(1,16):\n",
    "    pipes[degree] = Pipeline([('poly', PolynomialFeatures(degree=degree)), ('ols', LinearRegression())])\n",
    "    pipes[degree].fit(lsop_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for degree, color in [(1,'g'), (2,'r'), (3,'y'), (5,'c'), (13,'m')]:\n",
    "    plt.plot(curve_x, pipes[degree].predict(curve_x), color, lw=2, label=degree)\n",
    "plt.scatter(lsop_train, y_train, s=10)\n",
    "plt.xlabel(\"% lower status of the population\")\n",
    "plt.ylabel(\"Median value of owner-occupied homes in $1000's\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylim([0,60])\n",
    "plt.xlim([-10,50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge = Pipeline([('poly', PolynomialFeatures(degree=5)), ('ridge', Ridge())])\n",
    "params = {'ridge__alpha': np.logspace(-15, 13, 29)}\n",
    "rgrid_search = GridSearchCV(ridge, params, n_jobs=2, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rgrid_search.fit(lsop_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print rgrid_search.best_params_\n",
    "print rgrid_search.best_estimator_.score(lsop_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the pipelines here too, to see how the regularization parameters \"deform\" the 5 degree polynomial we saw in the previous plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipes = {}\n",
    "for alpha in np.logspace(-15, 13, 29):\n",
    "    pipes[alpha] = Pipeline([('poly', PolynomialFeatures(degree=5)), ('ridge', Ridge(alpha=alpha))])\n",
    "    pipes[alpha].fit(lsop_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for alpha, color in [(1e-13,'g'), (1e-1,'r'), (1e1,'y'), (1e2,'c'), (1e10,'m')]:\n",
    "    plt.plot(curve_x, pipes[alpha].predict(curve_x), color, lw=2, label=alpha)\n",
    "plt.scatter(lsop_train, y_train, s=10)\n",
    "plt.xlabel(\"% lower status of the population\")\n",
    "plt.ylabel(\"Median value of owner-occupied homes in $1000's\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylim([0,60])\n",
    "plt.xlim([-10,50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## LASSO\n",
    "\n",
    "Least absolute shrinkage and selection operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lasso = Pipeline([('poly', PolynomialFeatures(degree=5)), ('lasso', Lasso(max_iter=10000))])\n",
    "params = {'lasso__alpha': np.logspace(-5, 13, 19)}\n",
    "lgrid_search = GridSearchCV(lasso, params, scoring='neg_mean_squared_error')\n",
    "lgrid_search.fit(lsop_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lgrid_search.best_params_, -lgrid_search.score(lsop_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASSO also works as a feature selection tool, we can see that by setting the alpha high enough, it sets some coefficients to zero. Also, we can see that if we go overboard with this, it can lead to **underfitting**, which is also bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipes = {}\n",
    "coefs = pd.DataFrame()\n",
    "for alpha in np.logspace(-5, 13, 19):\n",
    "    pipes[alpha] = Pipeline([('poly', PolynomialFeatures(degree=5)), ('lasso', Lasso(max_iter=100000, alpha=alpha))])\n",
    "    pipes[alpha].fit(lsop_train, y_train)\n",
    "    coefs[alpha] = pipes[alpha].named_steps['lasso'].coef_[1:]\n",
    "    \n",
    "print coefs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for alpha, color in [(1e-5,'g'), (1e-2,'r'), (1e-1,'y'), (1e1,'c'), (1e7,'m')]:\n",
    "    plt.plot(curve_x, pipes[alpha].predict(curve_x), color, lw=2, label=alpha)\n",
    "plt.scatter(lsop_train, y_train, s=10)\n",
    "plt.xlabel(\"% lower status of the population\")\n",
    "plt.ylabel(\"Median value of owner-occupied homes in $1000's\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylim([0,60])\n",
    "plt.xlim([-10,50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import BayesianRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b_ridge = Pipeline([('poly', PolynomialFeatures(degree=5)), ('b_ridge', BayesianRidge())])\n",
    "b_ridge.fit(lsop_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_squared_error(b_ridge.predict(lsop_test), y_test), b_ridge.score(lsop_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svr = Pipeline([('svr', SVR())])\n",
    "svr.fit(lsop_train, y_train)\n",
    "mean_squared_error(svr.predict(lsop_test), y_test), svr.score(lsop_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
