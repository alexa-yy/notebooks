{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Data Science @ SzISz\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "- <a href=\"#Administration\">Administration</a>\n",
    "- <a href=\"#Intro\">Intro</a>\n",
    "- <a href=\"#Overview\">Overview</a>\n",
    "- <a href=\"#Regression\">Regression</a>\n",
    "\n",
    "## Administration\n",
    "\n",
    "### Curriculum:\n",
    "- Overview, technical basics, regression\n",
    "- Data Discovery, Naive linear classifiers\n",
    "- Data Transformation, Decision trees\n",
    "- Dimensionality Reduction\n",
    "- ...\n",
    "\n",
    "### Requirements:\n",
    "\n",
    "A selected project submitted to one of \n",
    "<a href=\"https://www.kaggle.com/competitions\">kaggle.com</a>'s competitions.\n",
    "\n",
    "## Intro\n",
    "\n",
    "### WTF is Data Science?\n",
    "\n",
    "According to a random venn diagram:\n",
    "\n",
    "<img src=\"http://b-i.forbesimg.com/gilpress/files/2013/05/Data_Science_VD.png\" width=300 align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a metro map: \n",
    "\n",
    "<a href=\"http://nirvacana.com/thoughts/wp-content/uploads/2013/07/RoadToDataScientist1.png\" target=\"new\">\n",
    "    <img src=\"http://nirvacana.com/thoughts/wp-content/uploads/2013/07/RoadToDataScientist1.png\" width=500 align=\"left\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At the end of the day:\n",
    "\n",
    "It's just a fancier name for Data Mining. Maybe throw some more hacking skill to the mix.\n",
    "\n",
    "\n",
    "### Who is a Data Scientist then?\n",
    "\n",
    "- \"A data scientist is a statistician who lives in San Francisco\"\n",
    "- \"A data scientist is someone who is better at statistics than any software engineer and better at software engineering than any statistician.\"\n",
    "\n",
    "\n",
    "### Thanks, much clearer now. (NOT) Can you please tell me at least what does he do? \n",
    "#### A.k.a: the typical workflow - The Knowledge Discovery Process\n",
    "\n",
    "<img src=\"http://www.cs.utexas.edu/users/csed/doc_consortium/DC99/wooley-image1.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "There is a lot of \"implicit\" information in the data which humans can't directly observe, but can be extracted by statistic methods (a.k.a. _analytics_). Our goal is exactly this. Basically, there are two main types of analytics:\n",
    "\n",
    "### Descriptive analytics\n",
    "\n",
    "**Goal:** To extract the most valuable information from a given dataset.  \n",
    "**Example:** A store has some information on its customers, and from that information it can determine what type of people visit its stores (like students, retirees, etc.). This way it can adjust the stores open hours to fit the need of the different group of customers it serves. (This is called clustering.)\n",
    "\n",
    "### Predictive analytics\n",
    "\n",
    "**Goal:** Being able to make predictions on missing information based on previous knowledge.  \n",
    "**Example:** When you apply for a loan, the bank gets your data, and puts it into its model for predicting the probability of you repaying that loan. Depending on this prediction it can choose to grant you the loan you asked for or not.\n",
    "  \n",
    "---\n",
    "  \n",
    "There is another way of categorizing the methods that we will learn about in this course: **supervised** and **unsupervised** learning.  \n",
    "**Supervised learning** is based on data that is already 'labeled'. In other words we have data for which we know what the correct output is. We train our model on this dataset, and after this our model can predict the output of any input we give it. The simplest supervised learning method is the linear regression.  \n",
    "With **unsupervised learning** we don't know what the correct output should be - we try to detect structure in the data. The simplest example for this is descriptive statistics, or the above clustering example.\n",
    "\n",
    "### Validation\n",
    "\n",
    "How can we validate our model/output? In the case of unsupervised learning, we can't. With supervised learning, however the basic idea is pretty straightforward. We split our dataset into two parts: training and test set. We train our model _only on the training set_, and then compare the model's output on the test set to the known good output on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression, and a little scikit-learn\n",
    "\n",
    "<img src=\"http://vignette2.wikia.nocookie.net/nickelodeon/images/1/14/Ren%2B%2BStimpy.jpg\" width=200 align=left>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, generate the data and add some noise to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get 100 samples from a normal distribution\n",
    "n_samples = 100\n",
    "np.random.seed(0)\n",
    "X = np.random.normal(size=n_samples)\n",
    "\n",
    "# y is the signum function of X\n",
    "y = (X > 0).astype(np.float) \n",
    "\n",
    "# now add some noise to X\n",
    "X[X > 0] *= 4 \n",
    "X += .3 * np.random.normal(size=n_samples)\n",
    "\n",
    "# reshape X to be a 100 x 1 \"matrix\", or a column vector\n",
    "X = X[:, np.newaxis] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now introducing pipelines\n",
    "\n",
    "Since we only want a logistic regression in our model, we could simply use the LogisticRegression() function from the imported linear_model module. However, there is a useful concept called **pipeline**, which really comes in handy when dealing with more complicated models.\n",
    "\n",
    "When dealing with data, we may first want to transform our data to make it more digestible to our estimators (e.g. getting rid of some attributes). There can be multiple transformation steps involved in our process, and each transformation may have multiple parameters that can be tweaked independently. Pipelines provide a wrapping for these steps which makes working with these transformations easier and more conscise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In this example, we only want a logistic regression\n",
    "logistic = linear_model.LogisticRegression()\n",
    "pipe = Pipeline(steps=[('logistic', logistic)])\n",
    "\n",
    "# However, there is a regularization parameter C that can be choosen freely for our logistic regression estimator\n",
    "# Here we define what values of C should we try to make our model with\n",
    "Cs = np.logspace(-4, 4, 3) # C = [0.0001, 1.0, 10000]\n",
    "\n",
    "# GridSearchCV is the tool to try all the combinations of freely choosen parameters with our model\n",
    "# Now we only change one parameter, but with every new parameter we multiply the number of cases to try\n",
    "# So if we got another parameter with 4 values to try, there would be 12 cases altogether\n",
    "estimator = GridSearchCV(pipe, dict(logistic__C=Cs))\n",
    "estimator.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can see what the results of the different models were:\n",
    "estimator.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to see how our best model behaves\n",
    "X_test = np.linspace(-5, 10, 300)\n",
    "X_test = X_test[:, np.newaxis]\n",
    "\n",
    "bestimator = estimator.best_estimator_\n",
    "prediction = bestimator.predict(X_test)\n",
    "\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.clf()\n",
    "plt.plot(X_test.ravel(), prediction)\n",
    "plt.scatter(X.ravel(), y, color='black', zorder=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
